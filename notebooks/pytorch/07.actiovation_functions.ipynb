{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSOQUoZXBXvIt3Z8aO739o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://mohitmishra786687.medium.com/activation-functions-in-neural-networks-56dc526ad63c"],"metadata":{"id":"b5OTqThBr_V9"}},{"cell_type":"markdown","source":["# Activation Function\n","Activation functions apply a non-linear transformation and decide whether a neuron should be activated or not.\n","\n","- Without activation function, the neural network is basically just a stacked linear regression model.\n","\n","- A linear regression model predicts you a straight line that can only separae lineraly separable data.\n","\n","- If the true relationship is curved or complex, a straight line cannot classify it correctly.\n","\n","![activation](https://miro.medium.com/1*AsjGnS6iOgsA5RecS_ig5Q.png)\n","\n"],"metadata":{"id":"50-YDQSJd_rl"}},{"cell_type":"markdown","source":["# Popular Activation Function\n","- Step Function\n","- Sigmoid\n","- TanH\n","- ReLU\n","- Leaky ReLU\n","- Softmax"],"metadata":{"id":"Y9UagJ-ziJQg"}},{"cell_type":"markdown","source":["## Step Function\n","Not used in practice any more.\n","It basically tells whether a neuron shall be activated or not.\n","\n","![step](https://lh4.googleusercontent.com/proxy/EYhNGpiTTIdKyxjfWRdEnlsQJHOvvaeD0nOiXgVOr5XigTe3Meu5iCHuutEj5qmdyKzssXSzBNgNnYoQeNApw3g3ywg)\n","\n"],"metadata":{"id":"7z2ZYZ2jiaew"}},{"cell_type":"markdown","source":["## Sigmoid\n","It is used for binary classification.\n","\n","![sigmoid](https://media.geeksforgeeks.org/wp-content/uploads/20250131185746649092/Sigmoid-Activation-Function.png)"],"metadata":{"id":"b2qpnV6wjAh4"}},{"cell_type":"markdown","source":["## TanH\n","It is scaled sigmoid function. It is valued between -1 and 1. It is good choice for **hidden layer**.\n","\n","![tanh](https://miro.medium.com/v2/resize:fit:756/1*tOc--h-QU9_bHqWLPY9YLA.png)"],"metadata":{"id":"iefNaey2jOpl"}},{"cell_type":"markdown","source":["## ReLU\n","It is zero for negative, but linear for positive value. It is used for hidden layer. if you do not know which activation function to use, simply use ReLU.\n","\n","![relu](https://www.dailydoseofds.com/content/images/2023/06/relu-graph-1-1.jpeg)"],"metadata":{"id":"i9q-bhvpjao4"}},{"cell_type":"markdown","source":["# Leaky ReLU\n","\n","It solves the vanishing gradient problem.\n","\n","![leaky](https://www.i2tutorials.com/wp-content/media/2019/09/Deep-learning-25-i2tutorials.png)"],"metadata":{"id":"TmB6_seKXmRH"}},{"cell_type":"markdown","source":["# Softmax\n","\n","It is used for multi-class classification.\n","\n","![softmax](https://i0.wp.com/sefiks.com/wp-content/uploads/2017/11/softmax1.png?resize=850%2C329&ssl=1)"],"metadata":{"id":"QH4V2rjSXsiG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VB2lkC-ed_Gs"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# option1\n","class NeuralNet(nn.Module):\n","  def __init__(self, input_size, hidden_size):\n","    super(NeuralNet, self).__init__()\n","    self.linear1 = nn.Linear(input_size, hidden_size)\n","    self.relu = nn.ReLU()\n","    self.linear2 = nn.Linear(hidden_size, 1)\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, x):\n","    out = self.linear1(x)\n","    out = self.relu(out)\n","    out = self.linear2(out)\n","    out = self.sigmoid(out)\n","    return out\n","\n","# option2\n","class NeuralNet2(nn.Module):\n","  def __init__(self, input_size, hidden_size):\n","    super(NeuralNet, self).__init__()\n","    self.linear1 = nn.Linear(input_size, hidden_size)\n","    self.linear2 = nn.Linear(hidden_size, 1)\n","    self.relu = nn.ReLU()\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, x):\n","    out = torch.relu(self.linear1(x))\n","    out = torch.sigmoid(self.linear2(out))\n","    return out"]}]}