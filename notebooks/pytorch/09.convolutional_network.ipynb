{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPdBFxwIIa693v7rYQCBAth"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Convolutional Neural Network (CNN)\n","\n","A **Convolutional Neural Network (CNN)** is a type of neural network designed to handle **grid-like data**, most commonly **images**, but also audio or video. CNNs are especially good at recognizing **spatial patterns** like edges, textures, and shapes.\n","\n","## 1. Key Idea\n","- Uses **convolutional layers** that scan input with **small filters (kernels)**.\n","- Filters **slide across the input** and detect patterns like lines, curves, or textures.\n","- Automatically learns **which features are important** for tasks like image classification.\n","\n","## 2. Structure\n","### Convolutional Layer (Conv)\n","- Applies multiple filters to the input.\n","- Each filter produces a **feature map** showing where certain patterns appear.\n","- Example: A filter detecting vertical edges highlights vertical lines in an image.\n","\n","### Activation Function (e.g., ReLU)\n","- Adds non-linearity, allowing the network to learn complex patterns.\n","\n","### Pooling Layer (e.g., Max Pooling)\n","- Reduces the spatial size of feature maps.\n","- Keeps the most important information while reducing computation.\n","- Example: Max pooling picks the maximum value in a small region.\n","\n","### Fully Connected Layer\n","- After several convolution + pooling layers, features are flattened and fed into fully connected layers for classification or regression.\n","\n","## 3. How it Works\n","1. Input: 28x28 pixels grayscale image.\n","2. Convolution: Apply multiple 3x3 filters → produce feature maps.\n","3. ReLU: Make output non-linear.\n","4. Pooling: Downsample the feature maps.\n","5. Flatten → Fully Connected → Output probabilities for classes (e.g., digits 0–9).\n","\n","## 4. Advantages\n","- **Parameter sharing:** Same filter is used across the input → fewer weights than fully connected layers.\n","- **Translation invariance:** Can recognize patterns even if shifted slightly in the image.\n","- **Hierarchical feature learning:** Early layers learn edges, later layers learn shapes, and final layers learn object-level features.\n","\n","## 5. Visualization\n","```\n","Input Image (28x28)\n","       │\n","  [Conv 3x3 + ReLU] → Feature Map\n","       │\n","     [Pooling] → Downsampled Feature Map\n","       │\n","  [Conv 3x3 + ReLU] → More Complex Features\n","       │\n","     [Pooling] → Flatten\n","       │\n","  [Fully Connected Layer] → Output Classes\n","```\n","\n","\n","\n"],"metadata":{"id":"mgpiGl3V40kE"}},{"cell_type":"markdown","source":["## A Demo\n","This demo is to classify images from CIFAR-10 dataset. CIFAR-10 dataset has 10 different class of images."],"metadata":{"id":"3qroHYc_eBOX"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"WFP4pMxr3hmT","executionInfo":{"status":"ok","timestamp":1762701354691,"user_tz":480,"elapsed":8,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch._C import device\n","from torch.utils.data import Dataset, DataLoader\n","\n","# device config\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# -------------------------------------------------------------\n","# Hyperparameters\n","# -------------------------------------------------------------\n","input_size = 1024 # Because input CIFAR10 images are of 32 x 32 pixels\n","hidden_size = 128 # hidden layer neurons\n","num_classes = 10 # 10 different hand-written digits in MINST (0...9)\n","num_epochs = 4\n","batch_size = 4\n","learning_rate = 0.001"]},{"cell_type":"code","source":["# Load dataset\n","# Data augmentation + normalization for training set\n","# mean (0.4914, 0.4822, 0.4465) and std=(0.2470, 0.2435, 0.2616) are the industry standard mean & std for RGB channels\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),     # Randomly crop to 32x32 with padding\n","    transforms.RandomHorizontalFlip(),        # Randomly flip images horizontally\n","    transforms.ToTensor(),                    # Convert from [0,255] → [0.0,1.0]\n","    transforms.Normalize(\n","        mean=(0.4914, 0.4822, 0.4465),        # Channel-wise mean (R,G,B)\n","        std=(0.2470, 0.2435, 0.2616)          # Channel-wise std (R,G,B)\n","    ),\n","])\n","\n","# Only normalization for test/validation set\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=(0.4914, 0.4822, 0.4465),\n","        std=(0.2470, 0.2435, 0.2616)\n","    ),\n","])\n","\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform_train)\n","train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform_test)\n","test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"metadata":{"id":"2C-qgb4KjqIi","executionInfo":{"status":"ok","timestamp":1762701356240,"user_tz":480,"elapsed":1547,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# functions to show an image\n","\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","\n","# get some random training images\n","dataiter = iter(train_loader)\n","images, labels = next(dataiter)\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))\n","# print labels\n","print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":229},"id":"VQPzD072Hq8k","executionInfo":{"status":"ok","timestamp":1762701356456,"user_tz":480,"elapsed":212,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"901ec26e-a0a5-45f1-f3cd-f31f1cca090c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.5068768].\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["cat   frog  ship  horse\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# =============================================================\n","# Convolutional Neural Network (CNN) for CIFAR-10\n","# =============================================================\n","# CIFAR-10 images: 32x32 pixels, 3 channels (RGB)\n","# CNN preserves spatial structure and learns hierarchical features\n","# =============================================================\n","\n","class ConvNet(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(ConvNet, self).__init__()\n","\n","        # -------------------------\n","        # CONVOLUTIONAL BLOCK 1\n","        # -------------------------\n","        # Input: (B, 3, 32, 32)\n","        # Conv2d:\n","        #   in_channels = 3 (RGB)\n","        #   out_channels = 32 → number of filters → produces 32 feature maps\n","        #   kernel_size = 3 → size of each filter (3x3)\n","        #   padding = 1 → keeps spatial size same (32x32)\n","        # Output after conv: (B, 32, 32, 32)\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(32)  # preserves shape: (B, 32, 32, 32)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Downsample → (B, 32, 16, 16)\n","\n","        # -------------------------\n","        # CONVOLUTIONAL BLOCK 2\n","        # -------------------------\n","        # Input: (B, 32, 16, 16)\n","        # Conv2d:\n","        #   in_channels = 32\n","        #   out_channels = 64 → number of filters → produces 64 feature maps\n","        #   kernel_size = 3 → 3x3 filters\n","        #   padding = 1 → spatial size preserved (16x16)\n","        # Output after conv: (B, 64, 16, 16)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)  # preserves shape: (B, 64, 16, 16)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # Downsample → (B, 64, 8, 8)\n","\n","        # -------------------------\n","        # CONVOLUTIONAL BLOCK 3\n","        # -------------------------\n","        # Input: (B, 64, 8, 8)\n","        # Conv2d:\n","        #   in_channels = 64\n","        #   out_channels = 128 → number of filters → produces 128 feature maps\n","        #   kernel_size = 3 → 3x3 filters\n","        #   padding = 1 → spatial size preserved (8x8)\n","        # Output after conv: (B, 128, 8, 8)\n","        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)  # preserves shape: (B, 128, 8, 8)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # Downsample → (B, 128, 4, 4)\n","\n","        # -------------------------\n","        # FULLY CONNECTED LAYERS\n","        # -------------------------\n","        # Flatten (B, 128, 4, 4) → (B, 128*4*4 = 2048)\n","        self.fc1 = nn.Linear(128 * 4 * 4, 256)  # Output: (B, 256)\n","        self.fc2 = nn.Linear(256, num_classes)  # Output: (B, 10)\n","\n","    def forward(self, x):\n","        # -------------------------\n","        # Forward pass\n","        # -------------------------\n","        x = F.relu(self.bn1(self.conv1(x)))  # Conv1 + BN + ReLU → (B, 32, 32, 32)\n","        x = self.pool1(x)                    # MaxPool1 → (B, 32, 16, 16)\n","\n","        x = F.relu(self.bn2(self.conv2(x)))  # Conv2 + BN + ReLU → (B, 64, 16, 16)\n","        x = self.pool2(x)                    # MaxPool2 → (B, 64, 8, 8)\n","\n","        x = F.relu(self.bn3(self.conv3(x)))  # Conv3 + BN + ReLU → (B, 128, 8, 8)\n","        x = self.pool3(x)                    # MaxPool3 → (B, 128, 4, 4)\n","\n","        # Flatten feature maps to 1D vector → (B, 2048)\n","        x = x.view(x.size(0), -1)\n","\n","        # Fully connected layers\n","        x = F.relu(self.fc1(x))  # (B, 256)\n","        x = self.fc2(x)          # (B, 10) → logits for 10 classes\n","        return x\n","\n","\n","# =============================================================\n","# Model setup\n","# =============================================================\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_classes = 10\n","learning_rate = 0.001\n","num_epochs = 10\n","\n","model = ConvNet(num_classes=num_classes).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","\n","# =============================================================\n","# Training loop\n","# =============================================================\n","for epoch in range(num_epochs):\n","    model.train()\n","    for i, (images, labels) in enumerate(train_loader):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i + 1) % 100 == 0:\n","            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n","\n","\n","# =============================================================\n","# Evaluation loop\n","# =============================================================\n","model.eval()\n","with torch.no_grad():\n","    n_correct = 0\n","    n_samples = 0\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        outputs = model(images)\n","        _, predictions = torch.max(outputs, 1)\n","        n_samples += labels.size(0)\n","        n_correct += (predictions == labels).sum().item()\n","\n","    accuracy = 100 * n_correct / n_samples\n","    print(f'Accuracy on CIFAR-10 test images: {accuracy:.2f}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z8CaflSwr2pM","executionInfo":{"status":"ok","timestamp":1762702041583,"user_tz":480,"elapsed":685125,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"5e30acf6-6a1c-4e00-bc8f-73b893369f43"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Step [100/12500], Loss: 2.3873\n","Epoch [1/10], Step [200/12500], Loss: 2.0541\n","Epoch [1/10], Step [300/12500], Loss: 1.9906\n","Epoch [1/10], Step [400/12500], Loss: 1.6782\n","Epoch [1/10], Step [500/12500], Loss: 2.0611\n","Epoch [1/10], Step [600/12500], Loss: 2.0586\n","Epoch [1/10], Step [700/12500], Loss: 1.4399\n","Epoch [1/10], Step [800/12500], Loss: 1.7331\n","Epoch [1/10], Step [900/12500], Loss: 1.8004\n","Epoch [1/10], Step [1000/12500], Loss: 1.1341\n","Epoch [1/10], Step [1100/12500], Loss: 1.7960\n","Epoch [1/10], Step [1200/12500], Loss: 1.4527\n","Epoch [1/10], Step [1300/12500], Loss: 1.6482\n","Epoch [1/10], Step [1400/12500], Loss: 2.6435\n","Epoch [1/10], Step [1500/12500], Loss: 1.7688\n","Epoch [1/10], Step [1600/12500], Loss: 1.6700\n","Epoch [1/10], Step [1700/12500], Loss: 0.9796\n","Epoch [1/10], Step [1800/12500], Loss: 1.9316\n","Epoch [1/10], Step [1900/12500], Loss: 1.5131\n","Epoch [1/10], Step [2000/12500], Loss: 1.2306\n","Epoch [1/10], Step [2100/12500], Loss: 1.0011\n","Epoch [1/10], Step [2200/12500], Loss: 1.4870\n","Epoch [1/10], Step [2300/12500], Loss: 1.8572\n","Epoch [1/10], Step [2400/12500], Loss: 1.9418\n","Epoch [1/10], Step [2500/12500], Loss: 1.7398\n","Epoch [1/10], Step [2600/12500], Loss: 1.7291\n","Epoch [1/10], Step [2700/12500], Loss: 1.9214\n","Epoch [1/10], Step [2800/12500], Loss: 1.1214\n","Epoch [1/10], Step [2900/12500], Loss: 1.1784\n","Epoch [1/10], Step [3000/12500], Loss: 1.8121\n","Epoch [1/10], Step [3100/12500], Loss: 2.5561\n","Epoch [1/10], Step [3200/12500], Loss: 1.9742\n","Epoch [1/10], Step [3300/12500], Loss: 1.4877\n","Epoch [1/10], Step [3400/12500], Loss: 1.7651\n","Epoch [1/10], Step [3500/12500], Loss: 1.3694\n","Epoch [1/10], Step [3600/12500], Loss: 1.7401\n","Epoch [1/10], Step [3700/12500], Loss: 2.3359\n","Epoch [1/10], Step [3800/12500], Loss: 2.0215\n","Epoch [1/10], Step [3900/12500], Loss: 2.4081\n","Epoch [1/10], Step [4000/12500], Loss: 1.5171\n","Epoch [1/10], Step [4100/12500], Loss: 0.9992\n","Epoch [1/10], Step [4200/12500], Loss: 1.7687\n","Epoch [1/10], Step [4300/12500], Loss: 0.8153\n","Epoch [1/10], Step [4400/12500], Loss: 0.9545\n","Epoch [1/10], Step [4500/12500], Loss: 2.1811\n","Epoch [1/10], Step [4600/12500], Loss: 1.8001\n","Epoch [1/10], Step [4700/12500], Loss: 0.7956\n","Epoch [1/10], Step [4800/12500], Loss: 1.7452\n","Epoch [1/10], Step [4900/12500], Loss: 1.2111\n","Epoch [1/10], Step [5000/12500], Loss: 2.2287\n","Epoch [1/10], Step [5100/12500], Loss: 0.9366\n","Epoch [1/10], Step [5200/12500], Loss: 1.5323\n","Epoch [1/10], Step [5300/12500], Loss: 2.1808\n","Epoch [1/10], Step [5400/12500], Loss: 1.7579\n","Epoch [1/10], Step [5500/12500], Loss: 0.8729\n","Epoch [1/10], Step [5600/12500], Loss: 1.1912\n","Epoch [1/10], Step [5700/12500], Loss: 1.8926\n","Epoch [1/10], Step [5800/12500], Loss: 1.5021\n","Epoch [1/10], Step [5900/12500], Loss: 1.6435\n","Epoch [1/10], Step [6000/12500], Loss: 2.9063\n","Epoch [1/10], Step [6100/12500], Loss: 1.6755\n","Epoch [1/10], Step [6200/12500], Loss: 0.8753\n","Epoch [1/10], Step [6300/12500], Loss: 1.9003\n","Epoch [1/10], Step [6400/12500], Loss: 1.4163\n","Epoch [1/10], Step [6500/12500], Loss: 1.2109\n","Epoch [1/10], Step [6600/12500], Loss: 1.7124\n","Epoch [1/10], Step [6700/12500], Loss: 1.0149\n","Epoch [1/10], Step [6800/12500], Loss: 0.5306\n","Epoch [1/10], Step [6900/12500], Loss: 1.4045\n","Epoch [1/10], Step [7000/12500], Loss: 1.0770\n","Epoch [1/10], Step [7100/12500], Loss: 1.9803\n","Epoch [1/10], Step [7200/12500], Loss: 1.2661\n","Epoch [1/10], Step [7300/12500], Loss: 1.9080\n","Epoch [1/10], Step [7400/12500], Loss: 1.0066\n","Epoch [1/10], Step [7500/12500], Loss: 1.7550\n","Epoch [1/10], Step [7600/12500], Loss: 0.7604\n","Epoch [1/10], Step [7700/12500], Loss: 0.9719\n","Epoch [1/10], Step [7800/12500], Loss: 2.5606\n","Epoch [1/10], Step [7900/12500], Loss: 1.4903\n","Epoch [1/10], Step [8000/12500], Loss: 1.4477\n","Epoch [1/10], Step [8100/12500], Loss: 1.7008\n","Epoch [1/10], Step [8200/12500], Loss: 0.8269\n","Epoch [1/10], Step [8300/12500], Loss: 1.2996\n","Epoch [1/10], Step [8400/12500], Loss: 1.4245\n","Epoch [1/10], Step [8500/12500], Loss: 1.2327\n","Epoch [1/10], Step [8600/12500], Loss: 0.7972\n","Epoch [1/10], Step [8700/12500], Loss: 1.0519\n","Epoch [1/10], Step [8800/12500], Loss: 1.8813\n","Epoch [1/10], Step [8900/12500], Loss: 1.4830\n","Epoch [1/10], Step [9000/12500], Loss: 1.0801\n","Epoch [1/10], Step [9100/12500], Loss: 1.8409\n","Epoch [1/10], Step [9200/12500], Loss: 0.6486\n","Epoch [1/10], Step [9300/12500], Loss: 1.6056\n","Epoch [1/10], Step [9400/12500], Loss: 0.8306\n","Epoch [1/10], Step [9500/12500], Loss: 0.4290\n","Epoch [1/10], Step [9600/12500], Loss: 1.3377\n","Epoch [1/10], Step [9700/12500], Loss: 1.1061\n","Epoch [1/10], Step [9800/12500], Loss: 1.9059\n","Epoch [1/10], Step [9900/12500], Loss: 1.7803\n","Epoch [1/10], Step [10000/12500], Loss: 0.5485\n","Epoch [1/10], Step [10100/12500], Loss: 2.2432\n","Epoch [1/10], Step [10200/12500], Loss: 1.0179\n","Epoch [1/10], Step [10300/12500], Loss: 1.2452\n","Epoch [1/10], Step [10400/12500], Loss: 2.4940\n","Epoch [1/10], Step [10500/12500], Loss: 0.5984\n","Epoch [1/10], Step [10600/12500], Loss: 0.3336\n","Epoch [1/10], Step [10700/12500], Loss: 1.3275\n","Epoch [1/10], Step [10800/12500], Loss: 0.7088\n","Epoch [1/10], Step [10900/12500], Loss: 1.2590\n","Epoch [1/10], Step [11000/12500], Loss: 0.9867\n","Epoch [1/10], Step [11100/12500], Loss: 1.4718\n","Epoch [1/10], Step [11200/12500], Loss: 1.8634\n","Epoch [1/10], Step [11300/12500], Loss: 1.8443\n","Epoch [1/10], Step [11400/12500], Loss: 1.5531\n","Epoch [1/10], Step [11500/12500], Loss: 0.9387\n","Epoch [1/10], Step [11600/12500], Loss: 1.4952\n","Epoch [1/10], Step [11700/12500], Loss: 0.5574\n","Epoch [1/10], Step [11800/12500], Loss: 0.9773\n","Epoch [1/10], Step [11900/12500], Loss: 0.8413\n","Epoch [1/10], Step [12000/12500], Loss: 2.0788\n","Epoch [1/10], Step [12100/12500], Loss: 2.1927\n","Epoch [1/10], Step [12200/12500], Loss: 1.7123\n","Epoch [1/10], Step [12300/12500], Loss: 1.6913\n","Epoch [1/10], Step [12400/12500], Loss: 2.2862\n","Epoch [1/10], Step [12500/12500], Loss: 0.9104\n","Epoch [2/10], Step [100/12500], Loss: 0.9297\n","Epoch [2/10], Step [200/12500], Loss: 0.7372\n","Epoch [2/10], Step [300/12500], Loss: 1.6654\n","Epoch [2/10], Step [400/12500], Loss: 0.3302\n","Epoch [2/10], Step [500/12500], Loss: 1.0739\n","Epoch [2/10], Step [600/12500], Loss: 1.4435\n","Epoch [2/10], Step [700/12500], Loss: 0.8716\n","Epoch [2/10], Step [800/12500], Loss: 0.7915\n","Epoch [2/10], Step [900/12500], Loss: 1.4311\n","Epoch [2/10], Step [1000/12500], Loss: 1.6672\n","Epoch [2/10], Step [1100/12500], Loss: 1.2329\n","Epoch [2/10], Step [1200/12500], Loss: 1.4140\n","Epoch [2/10], Step [1300/12500], Loss: 2.7278\n","Epoch [2/10], Step [1400/12500], Loss: 1.3411\n","Epoch [2/10], Step [1500/12500], Loss: 1.3379\n","Epoch [2/10], Step [1600/12500], Loss: 1.6282\n","Epoch [2/10], Step [1700/12500], Loss: 1.0599\n","Epoch [2/10], Step [1800/12500], Loss: 0.4372\n","Epoch [2/10], Step [1900/12500], Loss: 1.9504\n","Epoch [2/10], Step [2000/12500], Loss: 0.8787\n","Epoch [2/10], Step [2100/12500], Loss: 1.2254\n","Epoch [2/10], Step [2200/12500], Loss: 0.8794\n","Epoch [2/10], Step [2300/12500], Loss: 1.5513\n","Epoch [2/10], Step [2400/12500], Loss: 1.6649\n","Epoch [2/10], Step [2500/12500], Loss: 1.8724\n","Epoch [2/10], Step [2600/12500], Loss: 1.4158\n","Epoch [2/10], Step [2700/12500], Loss: 1.8488\n","Epoch [2/10], Step [2800/12500], Loss: 1.0420\n","Epoch [2/10], Step [2900/12500], Loss: 1.4552\n","Epoch [2/10], Step [3000/12500], Loss: 0.6141\n","Epoch [2/10], Step [3100/12500], Loss: 1.1601\n","Epoch [2/10], Step [3200/12500], Loss: 0.8258\n","Epoch [2/10], Step [3300/12500], Loss: 0.9978\n","Epoch [2/10], Step [3400/12500], Loss: 0.2096\n","Epoch [2/10], Step [3500/12500], Loss: 1.3463\n","Epoch [2/10], Step [3600/12500], Loss: 2.0501\n","Epoch [2/10], Step [3700/12500], Loss: 1.2947\n","Epoch [2/10], Step [3800/12500], Loss: 0.7208\n","Epoch [2/10], Step [3900/12500], Loss: 0.6730\n","Epoch [2/10], Step [4000/12500], Loss: 1.6088\n","Epoch [2/10], Step [4100/12500], Loss: 1.4570\n","Epoch [2/10], Step [4200/12500], Loss: 0.4866\n","Epoch [2/10], Step [4300/12500], Loss: 0.8008\n","Epoch [2/10], Step [4400/12500], Loss: 0.8396\n","Epoch [2/10], Step [4500/12500], Loss: 1.3052\n","Epoch [2/10], Step [4600/12500], Loss: 0.1866\n","Epoch [2/10], Step [4700/12500], Loss: 1.2188\n","Epoch [2/10], Step [4800/12500], Loss: 2.0108\n","Epoch [2/10], Step [4900/12500], Loss: 1.5389\n","Epoch [2/10], Step [5000/12500], Loss: 0.5382\n","Epoch [2/10], Step [5100/12500], Loss: 1.0765\n","Epoch [2/10], Step [5200/12500], Loss: 0.6364\n","Epoch [2/10], Step [5300/12500], Loss: 0.7393\n","Epoch [2/10], Step [5400/12500], Loss: 0.9242\n","Epoch [2/10], Step [5500/12500], Loss: 0.6431\n","Epoch [2/10], Step [5600/12500], Loss: 0.4820\n","Epoch [2/10], Step [5700/12500], Loss: 0.8161\n","Epoch [2/10], Step [5800/12500], Loss: 0.8824\n","Epoch [2/10], Step [5900/12500], Loss: 0.8840\n","Epoch [2/10], Step [6000/12500], Loss: 0.6502\n","Epoch [2/10], Step [6100/12500], Loss: 1.3125\n","Epoch [2/10], Step [6200/12500], Loss: 2.1219\n","Epoch [2/10], Step [6300/12500], Loss: 0.6636\n","Epoch [2/10], Step [6400/12500], Loss: 1.7917\n","Epoch [2/10], Step [6500/12500], Loss: 0.5557\n","Epoch [2/10], Step [6600/12500], Loss: 1.0669\n","Epoch [2/10], Step [6700/12500], Loss: 1.8578\n","Epoch [2/10], Step [6800/12500], Loss: 0.3981\n","Epoch [2/10], Step [6900/12500], Loss: 0.4642\n","Epoch [2/10], Step [7000/12500], Loss: 0.4740\n","Epoch [2/10], Step [7100/12500], Loss: 1.3594\n","Epoch [2/10], Step [7200/12500], Loss: 1.3264\n","Epoch [2/10], Step [7300/12500], Loss: 0.3820\n","Epoch [2/10], Step [7400/12500], Loss: 0.6258\n","Epoch [2/10], Step [7500/12500], Loss: 0.7712\n","Epoch [2/10], Step [7600/12500], Loss: 0.9740\n","Epoch [2/10], Step [7700/12500], Loss: 1.6406\n","Epoch [2/10], Step [7800/12500], Loss: 0.7720\n","Epoch [2/10], Step [7900/12500], Loss: 0.9481\n","Epoch [2/10], Step [8000/12500], Loss: 0.7397\n","Epoch [2/10], Step [8100/12500], Loss: 1.0529\n","Epoch [2/10], Step [8200/12500], Loss: 0.7803\n","Epoch [2/10], Step [8300/12500], Loss: 1.0158\n","Epoch [2/10], Step [8400/12500], Loss: 1.1078\n","Epoch [2/10], Step [8500/12500], Loss: 1.0386\n","Epoch [2/10], Step [8600/12500], Loss: 2.2375\n","Epoch [2/10], Step [8700/12500], Loss: 0.6574\n","Epoch [2/10], Step [8800/12500], Loss: 1.0241\n","Epoch [2/10], Step [8900/12500], Loss: 0.9646\n","Epoch [2/10], Step [9000/12500], Loss: 1.1708\n","Epoch [2/10], Step [9100/12500], Loss: 1.2572\n","Epoch [2/10], Step [9200/12500], Loss: 0.4390\n","Epoch [2/10], Step [9300/12500], Loss: 1.8994\n","Epoch [2/10], Step [9400/12500], Loss: 0.5674\n","Epoch [2/10], Step [9500/12500], Loss: 0.7237\n","Epoch [2/10], Step [9600/12500], Loss: 1.1724\n","Epoch [2/10], Step [9700/12500], Loss: 1.0010\n","Epoch [2/10], Step [9800/12500], Loss: 1.0145\n","Epoch [2/10], Step [9900/12500], Loss: 0.8024\n","Epoch [2/10], Step [10000/12500], Loss: 1.3643\n","Epoch [2/10], Step [10100/12500], Loss: 1.2695\n","Epoch [2/10], Step [10200/12500], Loss: 0.9760\n","Epoch [2/10], Step [10300/12500], Loss: 0.9563\n","Epoch [2/10], Step [10400/12500], Loss: 1.6699\n","Epoch [2/10], Step [10500/12500], Loss: 0.3934\n","Epoch [2/10], Step [10600/12500], Loss: 1.0766\n","Epoch [2/10], Step [10700/12500], Loss: 0.4161\n","Epoch [2/10], Step [10800/12500], Loss: 0.6291\n","Epoch [2/10], Step [10900/12500], Loss: 0.2207\n","Epoch [2/10], Step [11000/12500], Loss: 1.2618\n","Epoch [2/10], Step [11100/12500], Loss: 1.1680\n","Epoch [2/10], Step [11200/12500], Loss: 0.8686\n","Epoch [2/10], Step [11300/12500], Loss: 1.1448\n","Epoch [2/10], Step [11400/12500], Loss: 0.5869\n","Epoch [2/10], Step [11500/12500], Loss: 1.2279\n","Epoch [2/10], Step [11600/12500], Loss: 0.3265\n","Epoch [2/10], Step [11700/12500], Loss: 1.9190\n","Epoch [2/10], Step [11800/12500], Loss: 0.7142\n","Epoch [2/10], Step [11900/12500], Loss: 1.1370\n","Epoch [2/10], Step [12000/12500], Loss: 0.6730\n","Epoch [2/10], Step [12100/12500], Loss: 1.9076\n","Epoch [2/10], Step [12200/12500], Loss: 1.2617\n","Epoch [2/10], Step [12300/12500], Loss: 0.8294\n","Epoch [2/10], Step [12400/12500], Loss: 0.7425\n","Epoch [2/10], Step [12500/12500], Loss: 1.3422\n","Epoch [3/10], Step [100/12500], Loss: 1.7200\n","Epoch [3/10], Step [200/12500], Loss: 0.3718\n","Epoch [3/10], Step [300/12500], Loss: 1.2597\n","Epoch [3/10], Step [400/12500], Loss: 0.5531\n","Epoch [3/10], Step [500/12500], Loss: 1.2854\n","Epoch [3/10], Step [600/12500], Loss: 0.8079\n","Epoch [3/10], Step [700/12500], Loss: 1.1269\n","Epoch [3/10], Step [800/12500], Loss: 0.2922\n","Epoch [3/10], Step [900/12500], Loss: 1.1208\n","Epoch [3/10], Step [1000/12500], Loss: 0.5351\n","Epoch [3/10], Step [1100/12500], Loss: 0.0808\n","Epoch [3/10], Step [1200/12500], Loss: 1.1099\n","Epoch [3/10], Step [1300/12500], Loss: 1.0670\n","Epoch [3/10], Step [1400/12500], Loss: 1.4966\n","Epoch [3/10], Step [1500/12500], Loss: 1.2691\n","Epoch [3/10], Step [1600/12500], Loss: 0.9259\n","Epoch [3/10], Step [1700/12500], Loss: 0.8848\n","Epoch [3/10], Step [1800/12500], Loss: 0.6188\n","Epoch [3/10], Step [1900/12500], Loss: 0.4398\n","Epoch [3/10], Step [2000/12500], Loss: 0.6184\n","Epoch [3/10], Step [2100/12500], Loss: 0.5108\n","Epoch [3/10], Step [2200/12500], Loss: 0.7717\n","Epoch [3/10], Step [2300/12500], Loss: 1.7002\n","Epoch [3/10], Step [2400/12500], Loss: 0.4454\n","Epoch [3/10], Step [2500/12500], Loss: 0.3072\n","Epoch [3/10], Step [2600/12500], Loss: 0.8053\n","Epoch [3/10], Step [2700/12500], Loss: 0.6278\n","Epoch [3/10], Step [2800/12500], Loss: 0.7929\n","Epoch [3/10], Step [2900/12500], Loss: 0.1274\n","Epoch [3/10], Step [3000/12500], Loss: 0.9570\n","Epoch [3/10], Step [3100/12500], Loss: 0.6645\n","Epoch [3/10], Step [3200/12500], Loss: 0.4035\n","Epoch [3/10], Step [3300/12500], Loss: 0.5171\n","Epoch [3/10], Step [3400/12500], Loss: 1.0865\n","Epoch [3/10], Step [3500/12500], Loss: 0.2955\n","Epoch [3/10], Step [3600/12500], Loss: 1.1924\n","Epoch [3/10], Step [3700/12500], Loss: 1.1186\n","Epoch [3/10], Step [3800/12500], Loss: 1.4350\n","Epoch [3/10], Step [3900/12500], Loss: 1.0051\n","Epoch [3/10], Step [4000/12500], Loss: 0.7221\n","Epoch [3/10], Step [4100/12500], Loss: 0.6772\n","Epoch [3/10], Step [4200/12500], Loss: 1.9689\n","Epoch [3/10], Step [4300/12500], Loss: 0.9545\n","Epoch [3/10], Step [4400/12500], Loss: 0.7463\n","Epoch [3/10], Step [4500/12500], Loss: 0.8061\n","Epoch [3/10], Step [4600/12500], Loss: 1.0990\n","Epoch [3/10], Step [4700/12500], Loss: 1.7125\n","Epoch [3/10], Step [4800/12500], Loss: 0.1618\n","Epoch [3/10], Step [4900/12500], Loss: 0.4585\n","Epoch [3/10], Step [5000/12500], Loss: 0.3087\n","Epoch [3/10], Step [5100/12500], Loss: 0.9827\n","Epoch [3/10], Step [5200/12500], Loss: 1.3775\n","Epoch [3/10], Step [5300/12500], Loss: 0.9388\n","Epoch [3/10], Step [5400/12500], Loss: 0.5244\n","Epoch [3/10], Step [5500/12500], Loss: 0.2087\n","Epoch [3/10], Step [5600/12500], Loss: 0.9694\n","Epoch [3/10], Step [5700/12500], Loss: 0.3184\n","Epoch [3/10], Step [5800/12500], Loss: 1.1809\n","Epoch [3/10], Step [5900/12500], Loss: 1.1732\n","Epoch [3/10], Step [6000/12500], Loss: 2.1628\n","Epoch [3/10], Step [6100/12500], Loss: 0.4204\n","Epoch [3/10], Step [6200/12500], Loss: 0.4040\n","Epoch [3/10], Step [6300/12500], Loss: 1.6012\n","Epoch [3/10], Step [6400/12500], Loss: 1.3631\n","Epoch [3/10], Step [6500/12500], Loss: 0.6269\n","Epoch [3/10], Step [6600/12500], Loss: 0.7709\n","Epoch [3/10], Step [6700/12500], Loss: 1.7152\n","Epoch [3/10], Step [6800/12500], Loss: 0.6070\n","Epoch [3/10], Step [6900/12500], Loss: 0.9882\n","Epoch [3/10], Step [7000/12500], Loss: 0.6824\n","Epoch [3/10], Step [7100/12500], Loss: 0.8020\n","Epoch [3/10], Step [7200/12500], Loss: 0.7050\n","Epoch [3/10], Step [7300/12500], Loss: 0.5858\n","Epoch [3/10], Step [7400/12500], Loss: 1.1189\n","Epoch [3/10], Step [7500/12500], Loss: 0.9717\n","Epoch [3/10], Step [7600/12500], Loss: 1.0902\n","Epoch [3/10], Step [7700/12500], Loss: 1.1046\n","Epoch [3/10], Step [7800/12500], Loss: 0.9017\n","Epoch [3/10], Step [7900/12500], Loss: 0.1803\n","Epoch [3/10], Step [8000/12500], Loss: 0.8825\n","Epoch [3/10], Step [8100/12500], Loss: 1.2161\n","Epoch [3/10], Step [8200/12500], Loss: 0.4254\n","Epoch [3/10], Step [8300/12500], Loss: 0.5807\n","Epoch [3/10], Step [8400/12500], Loss: 0.6678\n","Epoch [3/10], Step [8500/12500], Loss: 0.1856\n","Epoch [3/10], Step [8600/12500], Loss: 0.4077\n","Epoch [3/10], Step [8700/12500], Loss: 1.6260\n","Epoch [3/10], Step [8800/12500], Loss: 1.0652\n","Epoch [3/10], Step [8900/12500], Loss: 0.8547\n","Epoch [3/10], Step [9000/12500], Loss: 2.6393\n","Epoch [3/10], Step [9100/12500], Loss: 1.7132\n","Epoch [3/10], Step [9200/12500], Loss: 0.2847\n","Epoch [3/10], Step [9300/12500], Loss: 1.9163\n","Epoch [3/10], Step [9400/12500], Loss: 0.0546\n","Epoch [3/10], Step [9500/12500], Loss: 0.2485\n","Epoch [3/10], Step [9600/12500], Loss: 0.4857\n","Epoch [3/10], Step [9700/12500], Loss: 0.9441\n","Epoch [3/10], Step [9800/12500], Loss: 0.7265\n","Epoch [3/10], Step [9900/12500], Loss: 1.4011\n","Epoch [3/10], Step [10000/12500], Loss: 0.9393\n","Epoch [3/10], Step [10100/12500], Loss: 0.5760\n","Epoch [3/10], Step [10200/12500], Loss: 1.0773\n","Epoch [3/10], Step [10300/12500], Loss: 0.8426\n","Epoch [3/10], Step [10400/12500], Loss: 0.4828\n","Epoch [3/10], Step [10500/12500], Loss: 0.4787\n","Epoch [3/10], Step [10600/12500], Loss: 1.0589\n","Epoch [3/10], Step [10700/12500], Loss: 1.4679\n","Epoch [3/10], Step [10800/12500], Loss: 0.1311\n","Epoch [3/10], Step [10900/12500], Loss: 0.4668\n","Epoch [3/10], Step [11000/12500], Loss: 1.5795\n","Epoch [3/10], Step [11100/12500], Loss: 1.2539\n","Epoch [3/10], Step [11200/12500], Loss: 0.7867\n","Epoch [3/10], Step [11300/12500], Loss: 0.6150\n","Epoch [3/10], Step [11400/12500], Loss: 0.4846\n","Epoch [3/10], Step [11500/12500], Loss: 0.5527\n","Epoch [3/10], Step [11600/12500], Loss: 0.1437\n","Epoch [3/10], Step [11700/12500], Loss: 1.0246\n","Epoch [3/10], Step [11800/12500], Loss: 0.6368\n","Epoch [3/10], Step [11900/12500], Loss: 0.5813\n","Epoch [3/10], Step [12000/12500], Loss: 0.0930\n","Epoch [3/10], Step [12100/12500], Loss: 1.8333\n","Epoch [3/10], Step [12200/12500], Loss: 1.8884\n","Epoch [3/10], Step [12300/12500], Loss: 0.3168\n","Epoch [3/10], Step [12400/12500], Loss: 0.3331\n","Epoch [3/10], Step [12500/12500], Loss: 1.4997\n","Epoch [4/10], Step [100/12500], Loss: 0.7953\n","Epoch [4/10], Step [200/12500], Loss: 0.4563\n","Epoch [4/10], Step [300/12500], Loss: 1.0916\n","Epoch [4/10], Step [400/12500], Loss: 2.1855\n","Epoch [4/10], Step [500/12500], Loss: 0.2758\n","Epoch [4/10], Step [600/12500], Loss: 1.3229\n","Epoch [4/10], Step [700/12500], Loss: 0.5045\n","Epoch [4/10], Step [800/12500], Loss: 1.0002\n","Epoch [4/10], Step [900/12500], Loss: 1.4275\n","Epoch [4/10], Step [1000/12500], Loss: 0.7264\n","Epoch [4/10], Step [1100/12500], Loss: 1.0567\n","Epoch [4/10], Step [1200/12500], Loss: 0.5198\n","Epoch [4/10], Step [1300/12500], Loss: 1.6177\n","Epoch [4/10], Step [1400/12500], Loss: 0.9010\n","Epoch [4/10], Step [1500/12500], Loss: 1.2504\n","Epoch [4/10], Step [1600/12500], Loss: 0.8670\n","Epoch [4/10], Step [1700/12500], Loss: 1.3381\n","Epoch [4/10], Step [1800/12500], Loss: 0.8807\n","Epoch [4/10], Step [1900/12500], Loss: 2.1132\n","Epoch [4/10], Step [2000/12500], Loss: 1.1025\n","Epoch [4/10], Step [2100/12500], Loss: 0.5614\n","Epoch [4/10], Step [2200/12500], Loss: 0.8191\n","Epoch [4/10], Step [2300/12500], Loss: 1.3122\n","Epoch [4/10], Step [2400/12500], Loss: 0.9565\n","Epoch [4/10], Step [2500/12500], Loss: 0.3753\n","Epoch [4/10], Step [2600/12500], Loss: 1.4725\n","Epoch [4/10], Step [2700/12500], Loss: 1.5049\n","Epoch [4/10], Step [2800/12500], Loss: 0.4349\n","Epoch [4/10], Step [2900/12500], Loss: 1.4300\n","Epoch [4/10], Step [3000/12500], Loss: 1.4958\n","Epoch [4/10], Step [3100/12500], Loss: 0.3951\n","Epoch [4/10], Step [3200/12500], Loss: 0.3780\n","Epoch [4/10], Step [3300/12500], Loss: 0.6310\n","Epoch [4/10], Step [3400/12500], Loss: 1.6215\n","Epoch [4/10], Step [3500/12500], Loss: 1.0012\n","Epoch [4/10], Step [3600/12500], Loss: 1.9226\n","Epoch [4/10], Step [3700/12500], Loss: 0.3299\n","Epoch [4/10], Step [3800/12500], Loss: 0.9928\n","Epoch [4/10], Step [3900/12500], Loss: 0.2915\n","Epoch [4/10], Step [4000/12500], Loss: 0.2143\n","Epoch [4/10], Step [4100/12500], Loss: 1.7173\n","Epoch [4/10], Step [4200/12500], Loss: 2.1101\n","Epoch [4/10], Step [4300/12500], Loss: 0.3431\n","Epoch [4/10], Step [4400/12500], Loss: 0.5444\n","Epoch [4/10], Step [4500/12500], Loss: 0.8767\n","Epoch [4/10], Step [4600/12500], Loss: 0.4101\n","Epoch [4/10], Step [4700/12500], Loss: 0.4368\n","Epoch [4/10], Step [4800/12500], Loss: 1.4750\n","Epoch [4/10], Step [4900/12500], Loss: 0.5973\n","Epoch [4/10], Step [5000/12500], Loss: 0.5907\n","Epoch [4/10], Step [5100/12500], Loss: 0.2140\n","Epoch [4/10], Step [5200/12500], Loss: 1.2537\n","Epoch [4/10], Step [5300/12500], Loss: 0.8055\n","Epoch [4/10], Step [5400/12500], Loss: 1.7480\n","Epoch [4/10], Step [5500/12500], Loss: 2.4137\n","Epoch [4/10], Step [5600/12500], Loss: 0.5927\n","Epoch [4/10], Step [5700/12500], Loss: 1.5017\n","Epoch [4/10], Step [5800/12500], Loss: 0.3679\n","Epoch [4/10], Step [5900/12500], Loss: 1.8523\n","Epoch [4/10], Step [6000/12500], Loss: 0.2415\n","Epoch [4/10], Step [6100/12500], Loss: 0.9036\n","Epoch [4/10], Step [6200/12500], Loss: 0.7010\n","Epoch [4/10], Step [6300/12500], Loss: 1.6419\n","Epoch [4/10], Step [6400/12500], Loss: 0.4993\n","Epoch [4/10], Step [6500/12500], Loss: 2.1930\n","Epoch [4/10], Step [6600/12500], Loss: 0.7287\n","Epoch [4/10], Step [6700/12500], Loss: 1.0507\n","Epoch [4/10], Step [6800/12500], Loss: 0.1652\n","Epoch [4/10], Step [6900/12500], Loss: 0.9825\n","Epoch [4/10], Step [7000/12500], Loss: 1.0518\n","Epoch [4/10], Step [7100/12500], Loss: 0.7949\n","Epoch [4/10], Step [7200/12500], Loss: 0.4016\n","Epoch [4/10], Step [7300/12500], Loss: 0.7208\n","Epoch [4/10], Step [7400/12500], Loss: 1.2657\n","Epoch [4/10], Step [7500/12500], Loss: 0.7832\n","Epoch [4/10], Step [7600/12500], Loss: 1.6421\n","Epoch [4/10], Step [7700/12500], Loss: 0.5579\n","Epoch [4/10], Step [7800/12500], Loss: 0.7272\n","Epoch [4/10], Step [7900/12500], Loss: 0.7906\n","Epoch [4/10], Step [8000/12500], Loss: 0.3262\n","Epoch [4/10], Step [8100/12500], Loss: 0.8836\n","Epoch [4/10], Step [8200/12500], Loss: 0.6369\n","Epoch [4/10], Step [8300/12500], Loss: 1.0876\n","Epoch [4/10], Step [8400/12500], Loss: 0.4681\n","Epoch [4/10], Step [8500/12500], Loss: 1.5852\n","Epoch [4/10], Step [8600/12500], Loss: 0.7607\n","Epoch [4/10], Step [8700/12500], Loss: 1.3612\n","Epoch [4/10], Step [8800/12500], Loss: 0.1854\n","Epoch [4/10], Step [8900/12500], Loss: 0.4643\n","Epoch [4/10], Step [9000/12500], Loss: 0.5886\n","Epoch [4/10], Step [9100/12500], Loss: 0.2890\n","Epoch [4/10], Step [9200/12500], Loss: 1.4582\n","Epoch [4/10], Step [9300/12500], Loss: 1.0644\n","Epoch [4/10], Step [9400/12500], Loss: 0.7868\n","Epoch [4/10], Step [9500/12500], Loss: 0.5292\n","Epoch [4/10], Step [9600/12500], Loss: 0.0470\n","Epoch [4/10], Step [9700/12500], Loss: 0.1056\n","Epoch [4/10], Step [9800/12500], Loss: 1.0882\n","Epoch [4/10], Step [9900/12500], Loss: 0.5503\n","Epoch [4/10], Step [10000/12500], Loss: 0.5617\n","Epoch [4/10], Step [10100/12500], Loss: 0.5370\n","Epoch [4/10], Step [10200/12500], Loss: 1.8244\n","Epoch [4/10], Step [10300/12500], Loss: 0.0921\n","Epoch [4/10], Step [10400/12500], Loss: 0.2053\n","Epoch [4/10], Step [10500/12500], Loss: 0.2875\n","Epoch [4/10], Step [10600/12500], Loss: 0.0981\n","Epoch [4/10], Step [10700/12500], Loss: 1.4594\n","Epoch [4/10], Step [10800/12500], Loss: 0.2149\n","Epoch [4/10], Step [10900/12500], Loss: 0.8915\n","Epoch [4/10], Step [11000/12500], Loss: 0.6962\n","Epoch [4/10], Step [11100/12500], Loss: 1.6086\n","Epoch [4/10], Step [11200/12500], Loss: 0.1638\n","Epoch [4/10], Step [11300/12500], Loss: 1.0386\n","Epoch [4/10], Step [11400/12500], Loss: 0.4018\n","Epoch [4/10], Step [11500/12500], Loss: 1.9501\n","Epoch [4/10], Step [11600/12500], Loss: 0.2860\n","Epoch [4/10], Step [11700/12500], Loss: 0.2843\n","Epoch [4/10], Step [11800/12500], Loss: 0.3630\n","Epoch [4/10], Step [11900/12500], Loss: 0.7975\n","Epoch [4/10], Step [12000/12500], Loss: 1.0605\n","Epoch [4/10], Step [12100/12500], Loss: 0.5981\n","Epoch [4/10], Step [12200/12500], Loss: 1.5574\n","Epoch [4/10], Step [12300/12500], Loss: 0.7907\n","Epoch [4/10], Step [12400/12500], Loss: 1.9487\n","Epoch [4/10], Step [12500/12500], Loss: 0.1707\n","Epoch [5/10], Step [100/12500], Loss: 0.2586\n","Epoch [5/10], Step [200/12500], Loss: 1.4224\n","Epoch [5/10], Step [300/12500], Loss: 0.8400\n","Epoch [5/10], Step [400/12500], Loss: 0.5897\n","Epoch [5/10], Step [500/12500], Loss: 0.7331\n","Epoch [5/10], Step [600/12500], Loss: 1.0790\n","Epoch [5/10], Step [700/12500], Loss: 1.1514\n","Epoch [5/10], Step [800/12500], Loss: 1.2198\n","Epoch [5/10], Step [900/12500], Loss: 1.8385\n","Epoch [5/10], Step [1000/12500], Loss: 1.2523\n","Epoch [5/10], Step [1100/12500], Loss: 1.6642\n","Epoch [5/10], Step [1200/12500], Loss: 0.8590\n","Epoch [5/10], Step [1300/12500], Loss: 0.9703\n","Epoch [5/10], Step [1400/12500], Loss: 1.5920\n","Epoch [5/10], Step [1500/12500], Loss: 0.2824\n","Epoch [5/10], Step [1600/12500], Loss: 0.7887\n","Epoch [5/10], Step [1700/12500], Loss: 1.3192\n","Epoch [5/10], Step [1800/12500], Loss: 0.2723\n","Epoch [5/10], Step [1900/12500], Loss: 1.8283\n","Epoch [5/10], Step [2000/12500], Loss: 0.2889\n","Epoch [5/10], Step [2100/12500], Loss: 0.4040\n","Epoch [5/10], Step [2200/12500], Loss: 0.9268\n","Epoch [5/10], Step [2300/12500], Loss: 0.4954\n","Epoch [5/10], Step [2400/12500], Loss: 0.8590\n","Epoch [5/10], Step [2500/12500], Loss: 1.4568\n","Epoch [5/10], Step [2600/12500], Loss: 2.3394\n","Epoch [5/10], Step [2700/12500], Loss: 0.3750\n","Epoch [5/10], Step [2800/12500], Loss: 0.5617\n","Epoch [5/10], Step [2900/12500], Loss: 1.5554\n","Epoch [5/10], Step [3000/12500], Loss: 0.4250\n","Epoch [5/10], Step [3100/12500], Loss: 0.4529\n","Epoch [5/10], Step [3200/12500], Loss: 0.7790\n","Epoch [5/10], Step [3300/12500], Loss: 0.5733\n","Epoch [5/10], Step [3400/12500], Loss: 0.3101\n","Epoch [5/10], Step [3500/12500], Loss: 0.9735\n","Epoch [5/10], Step [3600/12500], Loss: 0.3947\n","Epoch [5/10], Step [3700/12500], Loss: 1.1610\n","Epoch [5/10], Step [3800/12500], Loss: 0.2533\n","Epoch [5/10], Step [3900/12500], Loss: 0.9239\n","Epoch [5/10], Step [4000/12500], Loss: 0.7046\n","Epoch [5/10], Step [4100/12500], Loss: 1.5055\n","Epoch [5/10], Step [4200/12500], Loss: 0.8174\n","Epoch [5/10], Step [4300/12500], Loss: 0.7645\n","Epoch [5/10], Step [4400/12500], Loss: 0.5788\n","Epoch [5/10], Step [4500/12500], Loss: 0.5352\n","Epoch [5/10], Step [4600/12500], Loss: 1.0229\n","Epoch [5/10], Step [4700/12500], Loss: 0.8633\n","Epoch [5/10], Step [4800/12500], Loss: 1.1336\n","Epoch [5/10], Step [4900/12500], Loss: 0.5973\n","Epoch [5/10], Step [5000/12500], Loss: 0.3876\n","Epoch [5/10], Step [5100/12500], Loss: 3.2110\n","Epoch [5/10], Step [5200/12500], Loss: 1.4252\n","Epoch [5/10], Step [5300/12500], Loss: 0.3610\n","Epoch [5/10], Step [5400/12500], Loss: 0.6782\n","Epoch [5/10], Step [5500/12500], Loss: 0.2803\n","Epoch [5/10], Step [5600/12500], Loss: 1.1327\n","Epoch [5/10], Step [5700/12500], Loss: 0.1469\n","Epoch [5/10], Step [5800/12500], Loss: 1.0569\n","Epoch [5/10], Step [5900/12500], Loss: 0.5618\n","Epoch [5/10], Step [6000/12500], Loss: 1.4381\n","Epoch [5/10], Step [6100/12500], Loss: 0.6388\n","Epoch [5/10], Step [6200/12500], Loss: 2.5396\n","Epoch [5/10], Step [6300/12500], Loss: 0.4176\n","Epoch [5/10], Step [6400/12500], Loss: 2.0141\n","Epoch [5/10], Step [6500/12500], Loss: 0.5205\n","Epoch [5/10], Step [6600/12500], Loss: 0.6664\n","Epoch [5/10], Step [6700/12500], Loss: 0.7583\n","Epoch [5/10], Step [6800/12500], Loss: 0.4837\n","Epoch [5/10], Step [6900/12500], Loss: 0.8207\n","Epoch [5/10], Step [7000/12500], Loss: 0.7459\n","Epoch [5/10], Step [7100/12500], Loss: 0.6649\n","Epoch [5/10], Step [7200/12500], Loss: 1.1199\n","Epoch [5/10], Step [7300/12500], Loss: 0.9331\n","Epoch [5/10], Step [7400/12500], Loss: 0.6275\n","Epoch [5/10], Step [7500/12500], Loss: 0.5386\n","Epoch [5/10], Step [7600/12500], Loss: 0.2698\n","Epoch [5/10], Step [7700/12500], Loss: 0.3958\n","Epoch [5/10], Step [7800/12500], Loss: 0.6126\n","Epoch [5/10], Step [7900/12500], Loss: 1.2181\n","Epoch [5/10], Step [8000/12500], Loss: 0.6382\n","Epoch [5/10], Step [8100/12500], Loss: 0.0862\n","Epoch [5/10], Step [8200/12500], Loss: 0.9248\n","Epoch [5/10], Step [8300/12500], Loss: 0.2396\n","Epoch [5/10], Step [8400/12500], Loss: 0.5343\n","Epoch [5/10], Step [8500/12500], Loss: 0.7164\n","Epoch [5/10], Step [8600/12500], Loss: 0.2301\n","Epoch [5/10], Step [8700/12500], Loss: 0.6151\n","Epoch [5/10], Step [8800/12500], Loss: 1.2850\n","Epoch [5/10], Step [8900/12500], Loss: 1.8084\n","Epoch [5/10], Step [9000/12500], Loss: 1.6453\n","Epoch [5/10], Step [9100/12500], Loss: 1.6003\n","Epoch [5/10], Step [9200/12500], Loss: 1.3705\n","Epoch [5/10], Step [9300/12500], Loss: 0.6235\n","Epoch [5/10], Step [9400/12500], Loss: 1.8242\n","Epoch [5/10], Step [9500/12500], Loss: 0.0806\n","Epoch [5/10], Step [9600/12500], Loss: 1.6928\n","Epoch [5/10], Step [9700/12500], Loss: 1.1588\n","Epoch [5/10], Step [9800/12500], Loss: 0.1806\n","Epoch [5/10], Step [9900/12500], Loss: 0.8498\n","Epoch [5/10], Step [10000/12500], Loss: 0.3311\n","Epoch [5/10], Step [10100/12500], Loss: 2.4782\n","Epoch [5/10], Step [10200/12500], Loss: 3.0395\n","Epoch [5/10], Step [10300/12500], Loss: 1.9418\n","Epoch [5/10], Step [10400/12500], Loss: 0.9399\n","Epoch [5/10], Step [10500/12500], Loss: 1.2477\n","Epoch [5/10], Step [10600/12500], Loss: 0.3840\n","Epoch [5/10], Step [10700/12500], Loss: 0.3219\n","Epoch [5/10], Step [10800/12500], Loss: 2.2705\n","Epoch [5/10], Step [10900/12500], Loss: 0.7902\n","Epoch [5/10], Step [11000/12500], Loss: 0.1479\n","Epoch [5/10], Step [11100/12500], Loss: 0.1975\n","Epoch [5/10], Step [11200/12500], Loss: 0.9149\n","Epoch [5/10], Step [11300/12500], Loss: 1.9498\n","Epoch [5/10], Step [11400/12500], Loss: 0.0674\n","Epoch [5/10], Step [11500/12500], Loss: 0.8911\n","Epoch [5/10], Step [11600/12500], Loss: 0.7469\n","Epoch [5/10], Step [11700/12500], Loss: 0.3608\n","Epoch [5/10], Step [11800/12500], Loss: 1.0483\n","Epoch [5/10], Step [11900/12500], Loss: 0.1262\n","Epoch [5/10], Step [12000/12500], Loss: 0.9762\n","Epoch [5/10], Step [12100/12500], Loss: 1.1719\n","Epoch [5/10], Step [12200/12500], Loss: 0.4177\n","Epoch [5/10], Step [12300/12500], Loss: 0.3858\n","Epoch [5/10], Step [12400/12500], Loss: 1.8412\n","Epoch [5/10], Step [12500/12500], Loss: 0.6969\n","Epoch [6/10], Step [100/12500], Loss: 0.5641\n","Epoch [6/10], Step [200/12500], Loss: 1.7061\n","Epoch [6/10], Step [300/12500], Loss: 0.4731\n","Epoch [6/10], Step [400/12500], Loss: 0.2600\n","Epoch [6/10], Step [500/12500], Loss: 1.3917\n","Epoch [6/10], Step [600/12500], Loss: 0.9277\n","Epoch [6/10], Step [700/12500], Loss: 1.4394\n","Epoch [6/10], Step [800/12500], Loss: 0.7620\n","Epoch [6/10], Step [900/12500], Loss: 0.7634\n","Epoch [6/10], Step [1000/12500], Loss: 1.0148\n","Epoch [6/10], Step [1100/12500], Loss: 0.3893\n","Epoch [6/10], Step [1200/12500], Loss: 0.1484\n","Epoch [6/10], Step [1300/12500], Loss: 0.7874\n","Epoch [6/10], Step [1400/12500], Loss: 0.2736\n","Epoch [6/10], Step [1500/12500], Loss: 0.6606\n","Epoch [6/10], Step [1600/12500], Loss: 0.5750\n","Epoch [6/10], Step [1700/12500], Loss: 1.5724\n","Epoch [6/10], Step [1800/12500], Loss: 1.9633\n","Epoch [6/10], Step [1900/12500], Loss: 1.9109\n","Epoch [6/10], Step [2000/12500], Loss: 1.2157\n","Epoch [6/10], Step [2100/12500], Loss: 0.2636\n","Epoch [6/10], Step [2200/12500], Loss: 0.6025\n","Epoch [6/10], Step [2300/12500], Loss: 1.1036\n","Epoch [6/10], Step [2400/12500], Loss: 0.8087\n","Epoch [6/10], Step [2500/12500], Loss: 0.9825\n","Epoch [6/10], Step [2600/12500], Loss: 1.0285\n","Epoch [6/10], Step [2700/12500], Loss: 1.0787\n","Epoch [6/10], Step [2800/12500], Loss: 0.4205\n","Epoch [6/10], Step [2900/12500], Loss: 1.3618\n","Epoch [6/10], Step [3000/12500], Loss: 0.7557\n","Epoch [6/10], Step [3100/12500], Loss: 0.5741\n","Epoch [6/10], Step [3200/12500], Loss: 0.8152\n","Epoch [6/10], Step [3300/12500], Loss: 0.8445\n","Epoch [6/10], Step [3400/12500], Loss: 0.2311\n","Epoch [6/10], Step [3500/12500], Loss: 0.8045\n","Epoch [6/10], Step [3600/12500], Loss: 1.1451\n","Epoch [6/10], Step [3700/12500], Loss: 2.1490\n","Epoch [6/10], Step [3800/12500], Loss: 0.3540\n","Epoch [6/10], Step [3900/12500], Loss: 0.5103\n","Epoch [6/10], Step [4000/12500], Loss: 0.9027\n","Epoch [6/10], Step [4100/12500], Loss: 0.3096\n","Epoch [6/10], Step [4200/12500], Loss: 1.6923\n","Epoch [6/10], Step [4300/12500], Loss: 0.9216\n","Epoch [6/10], Step [4400/12500], Loss: 0.3645\n","Epoch [6/10], Step [4500/12500], Loss: 0.4169\n","Epoch [6/10], Step [4600/12500], Loss: 0.6145\n","Epoch [6/10], Step [4700/12500], Loss: 1.2212\n","Epoch [6/10], Step [4800/12500], Loss: 0.1554\n","Epoch [6/10], Step [4900/12500], Loss: 0.2596\n","Epoch [6/10], Step [5000/12500], Loss: 0.2968\n","Epoch [6/10], Step [5100/12500], Loss: 1.4100\n","Epoch [6/10], Step [5200/12500], Loss: 0.2487\n","Epoch [6/10], Step [5300/12500], Loss: 0.2423\n","Epoch [6/10], Step [5400/12500], Loss: 1.0688\n","Epoch [6/10], Step [5500/12500], Loss: 0.1977\n","Epoch [6/10], Step [5600/12500], Loss: 0.4055\n","Epoch [6/10], Step [5700/12500], Loss: 0.5147\n","Epoch [6/10], Step [5800/12500], Loss: 0.5576\n","Epoch [6/10], Step [5900/12500], Loss: 1.1290\n","Epoch [6/10], Step [6000/12500], Loss: 0.1475\n","Epoch [6/10], Step [6100/12500], Loss: 0.6408\n","Epoch [6/10], Step [6200/12500], Loss: 0.3953\n","Epoch [6/10], Step [6300/12500], Loss: 0.4836\n","Epoch [6/10], Step [6400/12500], Loss: 0.9039\n","Epoch [6/10], Step [6500/12500], Loss: 0.2507\n","Epoch [6/10], Step [6600/12500], Loss: 0.2817\n","Epoch [6/10], Step [6700/12500], Loss: 0.2616\n","Epoch [6/10], Step [6800/12500], Loss: 0.4551\n","Epoch [6/10], Step [6900/12500], Loss: 0.7932\n","Epoch [6/10], Step [7000/12500], Loss: 0.3722\n","Epoch [6/10], Step [7100/12500], Loss: 1.8547\n","Epoch [6/10], Step [7200/12500], Loss: 1.2646\n","Epoch [6/10], Step [7300/12500], Loss: 0.6465\n","Epoch [6/10], Step [7400/12500], Loss: 0.3643\n","Epoch [6/10], Step [7500/12500], Loss: 1.7310\n","Epoch [6/10], Step [7600/12500], Loss: 2.4042\n","Epoch [6/10], Step [7700/12500], Loss: 0.2356\n","Epoch [6/10], Step [7800/12500], Loss: 0.2987\n","Epoch [6/10], Step [7900/12500], Loss: 0.5911\n","Epoch [6/10], Step [8000/12500], Loss: 0.6241\n","Epoch [6/10], Step [8100/12500], Loss: 1.3760\n","Epoch [6/10], Step [8200/12500], Loss: 0.4786\n","Epoch [6/10], Step [8300/12500], Loss: 1.3564\n","Epoch [6/10], Step [8400/12500], Loss: 0.2781\n","Epoch [6/10], Step [8500/12500], Loss: 1.9005\n","Epoch [6/10], Step [8600/12500], Loss: 1.9332\n","Epoch [6/10], Step [8700/12500], Loss: 0.2903\n","Epoch [6/10], Step [8800/12500], Loss: 1.7457\n","Epoch [6/10], Step [8900/12500], Loss: 0.3326\n","Epoch [6/10], Step [9000/12500], Loss: 0.6840\n","Epoch [6/10], Step [9100/12500], Loss: 1.4597\n","Epoch [6/10], Step [9200/12500], Loss: 1.3617\n","Epoch [6/10], Step [9300/12500], Loss: 0.4136\n","Epoch [6/10], Step [9400/12500], Loss: 1.0435\n","Epoch [6/10], Step [9500/12500], Loss: 1.5088\n","Epoch [6/10], Step [9600/12500], Loss: 0.4997\n","Epoch [6/10], Step [9700/12500], Loss: 0.3688\n","Epoch [6/10], Step [9800/12500], Loss: 0.4090\n","Epoch [6/10], Step [9900/12500], Loss: 1.6907\n","Epoch [6/10], Step [10000/12500], Loss: 0.7091\n","Epoch [6/10], Step [10100/12500], Loss: 1.0882\n","Epoch [6/10], Step [10200/12500], Loss: 0.6987\n","Epoch [6/10], Step [10300/12500], Loss: 0.8248\n","Epoch [6/10], Step [10400/12500], Loss: 0.2130\n","Epoch [6/10], Step [10500/12500], Loss: 0.2077\n","Epoch [6/10], Step [10600/12500], Loss: 0.7443\n","Epoch [6/10], Step [10700/12500], Loss: 0.7340\n","Epoch [6/10], Step [10800/12500], Loss: 1.4270\n","Epoch [6/10], Step [10900/12500], Loss: 0.2503\n","Epoch [6/10], Step [11000/12500], Loss: 0.5008\n","Epoch [6/10], Step [11100/12500], Loss: 2.0720\n","Epoch [6/10], Step [11200/12500], Loss: 0.5475\n","Epoch [6/10], Step [11300/12500], Loss: 1.0525\n","Epoch [6/10], Step [11400/12500], Loss: 0.0738\n","Epoch [6/10], Step [11500/12500], Loss: 0.9912\n","Epoch [6/10], Step [11600/12500], Loss: 0.1013\n","Epoch [6/10], Step [11700/12500], Loss: 1.2698\n","Epoch [6/10], Step [11800/12500], Loss: 0.3415\n","Epoch [6/10], Step [11900/12500], Loss: 1.1565\n","Epoch [6/10], Step [12000/12500], Loss: 1.3491\n","Epoch [6/10], Step [12100/12500], Loss: 2.5793\n","Epoch [6/10], Step [12200/12500], Loss: 0.5231\n","Epoch [6/10], Step [12300/12500], Loss: 2.1408\n","Epoch [6/10], Step [12400/12500], Loss: 0.4372\n","Epoch [6/10], Step [12500/12500], Loss: 1.2367\n","Epoch [7/10], Step [100/12500], Loss: 0.9249\n","Epoch [7/10], Step [200/12500], Loss: 0.5877\n","Epoch [7/10], Step [300/12500], Loss: 0.1226\n","Epoch [7/10], Step [400/12500], Loss: 0.7619\n","Epoch [7/10], Step [500/12500], Loss: 0.8313\n","Epoch [7/10], Step [600/12500], Loss: 0.9103\n","Epoch [7/10], Step [700/12500], Loss: 0.9592\n","Epoch [7/10], Step [800/12500], Loss: 1.1835\n","Epoch [7/10], Step [900/12500], Loss: 0.0514\n","Epoch [7/10], Step [1000/12500], Loss: 0.5721\n","Epoch [7/10], Step [1100/12500], Loss: 1.4996\n","Epoch [7/10], Step [1200/12500], Loss: 0.4320\n","Epoch [7/10], Step [1300/12500], Loss: 1.4099\n","Epoch [7/10], Step [1400/12500], Loss: 0.3725\n","Epoch [7/10], Step [1500/12500], Loss: 0.2769\n","Epoch [7/10], Step [1600/12500], Loss: 0.9060\n","Epoch [7/10], Step [1700/12500], Loss: 1.5310\n","Epoch [7/10], Step [1800/12500], Loss: 0.4219\n","Epoch [7/10], Step [1900/12500], Loss: 0.7940\n","Epoch [7/10], Step [2000/12500], Loss: 1.6651\n","Epoch [7/10], Step [2100/12500], Loss: 0.5062\n","Epoch [7/10], Step [2200/12500], Loss: 0.7065\n","Epoch [7/10], Step [2300/12500], Loss: 0.3736\n","Epoch [7/10], Step [2400/12500], Loss: 0.3665\n","Epoch [7/10], Step [2500/12500], Loss: 0.9160\n","Epoch [7/10], Step [2600/12500], Loss: 0.5133\n","Epoch [7/10], Step [2700/12500], Loss: 0.1409\n","Epoch [7/10], Step [2800/12500], Loss: 0.2101\n","Epoch [7/10], Step [2900/12500], Loss: 0.9747\n","Epoch [7/10], Step [3000/12500], Loss: 1.5403\n","Epoch [7/10], Step [3100/12500], Loss: 0.6883\n","Epoch [7/10], Step [3200/12500], Loss: 0.9215\n","Epoch [7/10], Step [3300/12500], Loss: 0.1262\n","Epoch [7/10], Step [3400/12500], Loss: 0.2650\n","Epoch [7/10], Step [3500/12500], Loss: 1.4060\n","Epoch [7/10], Step [3600/12500], Loss: 1.0574\n","Epoch [7/10], Step [3700/12500], Loss: 0.7979\n","Epoch [7/10], Step [3800/12500], Loss: 0.2391\n","Epoch [7/10], Step [3900/12500], Loss: 0.7699\n","Epoch [7/10], Step [4000/12500], Loss: 0.2143\n","Epoch [7/10], Step [4100/12500], Loss: 0.2639\n","Epoch [7/10], Step [4200/12500], Loss: 0.7567\n","Epoch [7/10], Step [4300/12500], Loss: 0.7092\n","Epoch [7/10], Step [4400/12500], Loss: 0.8440\n","Epoch [7/10], Step [4500/12500], Loss: 0.2997\n","Epoch [7/10], Step [4600/12500], Loss: 0.5363\n","Epoch [7/10], Step [4700/12500], Loss: 0.3903\n","Epoch [7/10], Step [4800/12500], Loss: 0.5176\n","Epoch [7/10], Step [4900/12500], Loss: 0.4422\n","Epoch [7/10], Step [5000/12500], Loss: 0.8171\n","Epoch [7/10], Step [5100/12500], Loss: 0.3864\n","Epoch [7/10], Step [5200/12500], Loss: 0.5698\n","Epoch [7/10], Step [5300/12500], Loss: 0.6881\n","Epoch [7/10], Step [5400/12500], Loss: 0.1234\n","Epoch [7/10], Step [5500/12500], Loss: 0.5397\n","Epoch [7/10], Step [5600/12500], Loss: 1.6326\n","Epoch [7/10], Step [5700/12500], Loss: 0.3065\n","Epoch [7/10], Step [5800/12500], Loss: 0.4877\n","Epoch [7/10], Step [5900/12500], Loss: 1.2732\n","Epoch [7/10], Step [6000/12500], Loss: 0.5152\n","Epoch [7/10], Step [6100/12500], Loss: 0.8312\n","Epoch [7/10], Step [6200/12500], Loss: 0.3386\n","Epoch [7/10], Step [6300/12500], Loss: 0.7086\n","Epoch [7/10], Step [6400/12500], Loss: 0.8399\n","Epoch [7/10], Step [6500/12500], Loss: 1.5230\n","Epoch [7/10], Step [6600/12500], Loss: 0.5674\n","Epoch [7/10], Step [6700/12500], Loss: 0.6373\n","Epoch [7/10], Step [6800/12500], Loss: 1.1911\n","Epoch [7/10], Step [6900/12500], Loss: 0.1505\n","Epoch [7/10], Step [7000/12500], Loss: 0.8702\n","Epoch [7/10], Step [7100/12500], Loss: 0.1910\n","Epoch [7/10], Step [7200/12500], Loss: 0.9037\n","Epoch [7/10], Step [7300/12500], Loss: 0.3141\n","Epoch [7/10], Step [7400/12500], Loss: 0.2053\n","Epoch [7/10], Step [7500/12500], Loss: 0.6732\n","Epoch [7/10], Step [7600/12500], Loss: 0.0745\n","Epoch [7/10], Step [7700/12500], Loss: 1.1403\n","Epoch [7/10], Step [7800/12500], Loss: 0.4984\n","Epoch [7/10], Step [7900/12500], Loss: 0.7405\n","Epoch [7/10], Step [8000/12500], Loss: 0.7762\n","Epoch [7/10], Step [8100/12500], Loss: 0.1216\n","Epoch [7/10], Step [8200/12500], Loss: 0.4681\n","Epoch [7/10], Step [8300/12500], Loss: 0.8475\n","Epoch [7/10], Step [8400/12500], Loss: 0.1950\n","Epoch [7/10], Step [8500/12500], Loss: 0.5840\n","Epoch [7/10], Step [8600/12500], Loss: 0.9163\n","Epoch [7/10], Step [8700/12500], Loss: 0.4139\n","Epoch [7/10], Step [8800/12500], Loss: 0.9640\n","Epoch [7/10], Step [8900/12500], Loss: 1.3819\n","Epoch [7/10], Step [9000/12500], Loss: 0.9399\n","Epoch [7/10], Step [9100/12500], Loss: 1.0881\n","Epoch [7/10], Step [9200/12500], Loss: 0.4695\n","Epoch [7/10], Step [9300/12500], Loss: 0.3729\n","Epoch [7/10], Step [9400/12500], Loss: 0.5996\n","Epoch [7/10], Step [9500/12500], Loss: 1.6054\n","Epoch [7/10], Step [9600/12500], Loss: 0.8232\n","Epoch [7/10], Step [9700/12500], Loss: 1.4823\n","Epoch [7/10], Step [9800/12500], Loss: 1.3888\n","Epoch [7/10], Step [9900/12500], Loss: 1.8941\n","Epoch [7/10], Step [10000/12500], Loss: 1.5243\n","Epoch [7/10], Step [10100/12500], Loss: 0.3233\n","Epoch [7/10], Step [10200/12500], Loss: 0.4803\n","Epoch [7/10], Step [10300/12500], Loss: 0.1664\n","Epoch [7/10], Step [10400/12500], Loss: 0.5305\n","Epoch [7/10], Step [10500/12500], Loss: 0.6424\n","Epoch [7/10], Step [10600/12500], Loss: 1.8421\n","Epoch [7/10], Step [10700/12500], Loss: 1.3816\n","Epoch [7/10], Step [10800/12500], Loss: 0.5836\n","Epoch [7/10], Step [10900/12500], Loss: 0.7812\n","Epoch [7/10], Step [11000/12500], Loss: 1.1127\n","Epoch [7/10], Step [11100/12500], Loss: 0.2050\n","Epoch [7/10], Step [11200/12500], Loss: 0.9292\n","Epoch [7/10], Step [11300/12500], Loss: 0.1021\n","Epoch [7/10], Step [11400/12500], Loss: 1.2699\n","Epoch [7/10], Step [11500/12500], Loss: 0.1081\n","Epoch [7/10], Step [11600/12500], Loss: 1.7016\n","Epoch [7/10], Step [11700/12500], Loss: 0.5807\n","Epoch [7/10], Step [11800/12500], Loss: 0.1985\n","Epoch [7/10], Step [11900/12500], Loss: 2.2349\n","Epoch [7/10], Step [12000/12500], Loss: 0.4813\n","Epoch [7/10], Step [12100/12500], Loss: 0.0263\n","Epoch [7/10], Step [12200/12500], Loss: 0.1255\n","Epoch [7/10], Step [12300/12500], Loss: 0.8674\n","Epoch [7/10], Step [12400/12500], Loss: 1.5515\n","Epoch [7/10], Step [12500/12500], Loss: 1.2352\n","Epoch [8/10], Step [100/12500], Loss: 0.3024\n","Epoch [8/10], Step [200/12500], Loss: 0.8784\n","Epoch [8/10], Step [300/12500], Loss: 0.8066\n","Epoch [8/10], Step [400/12500], Loss: 0.4275\n","Epoch [8/10], Step [500/12500], Loss: 0.3647\n","Epoch [8/10], Step [600/12500], Loss: 1.2910\n","Epoch [8/10], Step [700/12500], Loss: 0.8787\n","Epoch [8/10], Step [800/12500], Loss: 0.4590\n","Epoch [8/10], Step [900/12500], Loss: 1.3068\n","Epoch [8/10], Step [1000/12500], Loss: 0.2681\n","Epoch [8/10], Step [1100/12500], Loss: 0.2183\n","Epoch [8/10], Step [1200/12500], Loss: 0.1619\n","Epoch [8/10], Step [1300/12500], Loss: 0.2934\n","Epoch [8/10], Step [1400/12500], Loss: 0.5067\n","Epoch [8/10], Step [1500/12500], Loss: 0.2782\n","Epoch [8/10], Step [1600/12500], Loss: 0.2900\n","Epoch [8/10], Step [1700/12500], Loss: 0.2417\n","Epoch [8/10], Step [1800/12500], Loss: 1.8747\n","Epoch [8/10], Step [1900/12500], Loss: 0.8302\n","Epoch [8/10], Step [2000/12500], Loss: 0.2185\n","Epoch [8/10], Step [2100/12500], Loss: 0.9637\n","Epoch [8/10], Step [2200/12500], Loss: 1.1867\n","Epoch [8/10], Step [2300/12500], Loss: 0.1477\n","Epoch [8/10], Step [2400/12500], Loss: 0.1229\n","Epoch [8/10], Step [2500/12500], Loss: 0.7137\n","Epoch [8/10], Step [2600/12500], Loss: 0.2831\n","Epoch [8/10], Step [2700/12500], Loss: 0.8882\n","Epoch [8/10], Step [2800/12500], Loss: 0.9078\n","Epoch [8/10], Step [2900/12500], Loss: 0.7040\n","Epoch [8/10], Step [3000/12500], Loss: 0.9868\n","Epoch [8/10], Step [3100/12500], Loss: 0.5047\n","Epoch [8/10], Step [3200/12500], Loss: 0.4992\n","Epoch [8/10], Step [3300/12500], Loss: 0.9007\n","Epoch [8/10], Step [3400/12500], Loss: 1.2523\n","Epoch [8/10], Step [3500/12500], Loss: 0.5905\n","Epoch [8/10], Step [3600/12500], Loss: 0.7268\n","Epoch [8/10], Step [3700/12500], Loss: 0.2295\n","Epoch [8/10], Step [3800/12500], Loss: 0.3200\n","Epoch [8/10], Step [3900/12500], Loss: 0.8424\n","Epoch [8/10], Step [4000/12500], Loss: 0.6444\n","Epoch [8/10], Step [4100/12500], Loss: 1.3995\n","Epoch [8/10], Step [4200/12500], Loss: 0.6342\n","Epoch [8/10], Step [4300/12500], Loss: 0.3923\n","Epoch [8/10], Step [4400/12500], Loss: 1.8125\n","Epoch [8/10], Step [4500/12500], Loss: 0.0611\n","Epoch [8/10], Step [4600/12500], Loss: 0.2119\n","Epoch [8/10], Step [4700/12500], Loss: 0.6989\n","Epoch [8/10], Step [4800/12500], Loss: 0.8430\n","Epoch [8/10], Step [4900/12500], Loss: 0.3452\n","Epoch [8/10], Step [5000/12500], Loss: 0.3336\n","Epoch [8/10], Step [5100/12500], Loss: 1.0147\n","Epoch [8/10], Step [5200/12500], Loss: 2.5675\n","Epoch [8/10], Step [5300/12500], Loss: 0.1640\n","Epoch [8/10], Step [5400/12500], Loss: 0.6075\n","Epoch [8/10], Step [5500/12500], Loss: 1.2038\n","Epoch [8/10], Step [5600/12500], Loss: 1.0589\n","Epoch [8/10], Step [5700/12500], Loss: 1.3172\n","Epoch [8/10], Step [5800/12500], Loss: 0.8390\n","Epoch [8/10], Step [5900/12500], Loss: 0.7777\n","Epoch [8/10], Step [6000/12500], Loss: 1.4787\n","Epoch [8/10], Step [6100/12500], Loss: 1.3042\n","Epoch [8/10], Step [6200/12500], Loss: 0.7340\n","Epoch [8/10], Step [6300/12500], Loss: 0.3415\n","Epoch [8/10], Step [6400/12500], Loss: 1.0264\n","Epoch [8/10], Step [6500/12500], Loss: 0.9049\n","Epoch [8/10], Step [6600/12500], Loss: 1.1400\n","Epoch [8/10], Step [6700/12500], Loss: 1.2391\n","Epoch [8/10], Step [6800/12500], Loss: 1.6763\n","Epoch [8/10], Step [6900/12500], Loss: 2.7989\n","Epoch [8/10], Step [7000/12500], Loss: 2.1543\n","Epoch [8/10], Step [7100/12500], Loss: 1.0354\n","Epoch [8/10], Step [7200/12500], Loss: 0.0188\n","Epoch [8/10], Step [7300/12500], Loss: 0.7134\n","Epoch [8/10], Step [7400/12500], Loss: 1.2447\n","Epoch [8/10], Step [7500/12500], Loss: 0.3471\n","Epoch [8/10], Step [7600/12500], Loss: 0.2326\n","Epoch [8/10], Step [7700/12500], Loss: 0.5845\n","Epoch [8/10], Step [7800/12500], Loss: 0.9030\n","Epoch [8/10], Step [7900/12500], Loss: 2.1830\n","Epoch [8/10], Step [8000/12500], Loss: 1.1314\n","Epoch [8/10], Step [8100/12500], Loss: 0.8485\n","Epoch [8/10], Step [8200/12500], Loss: 1.5946\n","Epoch [8/10], Step [8300/12500], Loss: 0.2960\n","Epoch [8/10], Step [8400/12500], Loss: 1.3892\n","Epoch [8/10], Step [8500/12500], Loss: 1.1814\n","Epoch [8/10], Step [8600/12500], Loss: 1.6367\n","Epoch [8/10], Step [8700/12500], Loss: 0.6493\n","Epoch [8/10], Step [8800/12500], Loss: 1.0621\n","Epoch [8/10], Step [8900/12500], Loss: 0.8424\n","Epoch [8/10], Step [9000/12500], Loss: 0.7774\n","Epoch [8/10], Step [9100/12500], Loss: 0.3552\n","Epoch [8/10], Step [9200/12500], Loss: 0.5533\n","Epoch [8/10], Step [9300/12500], Loss: 0.5439\n","Epoch [8/10], Step [9400/12500], Loss: 2.3090\n","Epoch [8/10], Step [9500/12500], Loss: 0.2473\n","Epoch [8/10], Step [9600/12500], Loss: 0.0480\n","Epoch [8/10], Step [9700/12500], Loss: 0.0593\n","Epoch [8/10], Step [9800/12500], Loss: 0.6681\n","Epoch [8/10], Step [9900/12500], Loss: 0.4207\n","Epoch [8/10], Step [10000/12500], Loss: 0.8231\n","Epoch [8/10], Step [10100/12500], Loss: 1.2320\n","Epoch [8/10], Step [10200/12500], Loss: 0.2930\n","Epoch [8/10], Step [10300/12500], Loss: 0.5209\n","Epoch [8/10], Step [10400/12500], Loss: 0.0693\n","Epoch [8/10], Step [10500/12500], Loss: 1.3328\n","Epoch [8/10], Step [10600/12500], Loss: 0.2518\n","Epoch [8/10], Step [10700/12500], Loss: 0.4195\n","Epoch [8/10], Step [10800/12500], Loss: 0.7001\n","Epoch [8/10], Step [10900/12500], Loss: 0.7066\n","Epoch [8/10], Step [11000/12500], Loss: 0.3334\n","Epoch [8/10], Step [11100/12500], Loss: 0.7658\n","Epoch [8/10], Step [11200/12500], Loss: 0.7894\n","Epoch [8/10], Step [11300/12500], Loss: 2.3383\n","Epoch [8/10], Step [11400/12500], Loss: 0.5209\n","Epoch [8/10], Step [11500/12500], Loss: 0.6239\n","Epoch [8/10], Step [11600/12500], Loss: 0.9339\n","Epoch [8/10], Step [11700/12500], Loss: 0.5351\n","Epoch [8/10], Step [11800/12500], Loss: 0.0614\n","Epoch [8/10], Step [11900/12500], Loss: 0.6505\n","Epoch [8/10], Step [12000/12500], Loss: 0.3789\n","Epoch [8/10], Step [12100/12500], Loss: 2.0455\n","Epoch [8/10], Step [12200/12500], Loss: 1.6225\n","Epoch [8/10], Step [12300/12500], Loss: 0.3416\n","Epoch [8/10], Step [12400/12500], Loss: 0.2598\n","Epoch [8/10], Step [12500/12500], Loss: 0.6952\n","Epoch [9/10], Step [100/12500], Loss: 0.9241\n","Epoch [9/10], Step [200/12500], Loss: 0.5706\n","Epoch [9/10], Step [300/12500], Loss: 0.7654\n","Epoch [9/10], Step [400/12500], Loss: 0.3344\n","Epoch [9/10], Step [500/12500], Loss: 0.8723\n","Epoch [9/10], Step [600/12500], Loss: 0.2115\n","Epoch [9/10], Step [700/12500], Loss: 0.1185\n","Epoch [9/10], Step [800/12500], Loss: 0.5604\n","Epoch [9/10], Step [900/12500], Loss: 1.5886\n","Epoch [9/10], Step [1000/12500], Loss: 0.2056\n","Epoch [9/10], Step [1100/12500], Loss: 0.3350\n","Epoch [9/10], Step [1200/12500], Loss: 1.4043\n","Epoch [9/10], Step [1300/12500], Loss: 0.2322\n","Epoch [9/10], Step [1400/12500], Loss: 0.8932\n","Epoch [9/10], Step [1500/12500], Loss: 0.2018\n","Epoch [9/10], Step [1600/12500], Loss: 0.4504\n","Epoch [9/10], Step [1700/12500], Loss: 0.3832\n","Epoch [9/10], Step [1800/12500], Loss: 0.5515\n","Epoch [9/10], Step [1900/12500], Loss: 1.1838\n","Epoch [9/10], Step [2000/12500], Loss: 1.0118\n","Epoch [9/10], Step [2100/12500], Loss: 0.3400\n","Epoch [9/10], Step [2200/12500], Loss: 0.9759\n","Epoch [9/10], Step [2300/12500], Loss: 0.6371\n","Epoch [9/10], Step [2400/12500], Loss: 0.1270\n","Epoch [9/10], Step [2500/12500], Loss: 0.3867\n","Epoch [9/10], Step [2600/12500], Loss: 2.0493\n","Epoch [9/10], Step [2700/12500], Loss: 0.7636\n","Epoch [9/10], Step [2800/12500], Loss: 0.7598\n","Epoch [9/10], Step [2900/12500], Loss: 1.0613\n","Epoch [9/10], Step [3000/12500], Loss: 0.1266\n","Epoch [9/10], Step [3100/12500], Loss: 0.0261\n","Epoch [9/10], Step [3200/12500], Loss: 1.2014\n","Epoch [9/10], Step [3300/12500], Loss: 0.3279\n","Epoch [9/10], Step [3400/12500], Loss: 0.6883\n","Epoch [9/10], Step [3500/12500], Loss: 0.4861\n","Epoch [9/10], Step [3600/12500], Loss: 1.4634\n","Epoch [9/10], Step [3700/12500], Loss: 1.3877\n","Epoch [9/10], Step [3800/12500], Loss: 2.1979\n","Epoch [9/10], Step [3900/12500], Loss: 0.5227\n","Epoch [9/10], Step [4000/12500], Loss: 0.1409\n","Epoch [9/10], Step [4100/12500], Loss: 1.1969\n","Epoch [9/10], Step [4200/12500], Loss: 1.0241\n","Epoch [9/10], Step [4300/12500], Loss: 0.9778\n","Epoch [9/10], Step [4400/12500], Loss: 0.0283\n","Epoch [9/10], Step [4500/12500], Loss: 0.2573\n","Epoch [9/10], Step [4600/12500], Loss: 0.2940\n","Epoch [9/10], Step [4700/12500], Loss: 1.0544\n","Epoch [9/10], Step [4800/12500], Loss: 0.3098\n","Epoch [9/10], Step [4900/12500], Loss: 0.0525\n","Epoch [9/10], Step [5000/12500], Loss: 0.0580\n","Epoch [9/10], Step [5100/12500], Loss: 0.7609\n","Epoch [9/10], Step [5200/12500], Loss: 0.0169\n","Epoch [9/10], Step [5300/12500], Loss: 1.7779\n","Epoch [9/10], Step [5400/12500], Loss: 1.1049\n","Epoch [9/10], Step [5500/12500], Loss: 0.2006\n","Epoch [9/10], Step [5600/12500], Loss: 1.2353\n","Epoch [9/10], Step [5700/12500], Loss: 0.1604\n","Epoch [9/10], Step [5800/12500], Loss: 0.7028\n","Epoch [9/10], Step [5900/12500], Loss: 0.2591\n","Epoch [9/10], Step [6000/12500], Loss: 0.5183\n","Epoch [9/10], Step [6100/12500], Loss: 1.1876\n","Epoch [9/10], Step [6200/12500], Loss: 1.0007\n","Epoch [9/10], Step [6300/12500], Loss: 0.1545\n","Epoch [9/10], Step [6400/12500], Loss: 0.3778\n","Epoch [9/10], Step [6500/12500], Loss: 0.3603\n","Epoch [9/10], Step [6600/12500], Loss: 1.4774\n","Epoch [9/10], Step [6700/12500], Loss: 0.7366\n","Epoch [9/10], Step [6800/12500], Loss: 0.7721\n","Epoch [9/10], Step [6900/12500], Loss: 0.5899\n","Epoch [9/10], Step [7000/12500], Loss: 0.4656\n","Epoch [9/10], Step [7100/12500], Loss: 0.2068\n","Epoch [9/10], Step [7200/12500], Loss: 0.4906\n","Epoch [9/10], Step [7300/12500], Loss: 0.0174\n","Epoch [9/10], Step [7400/12500], Loss: 1.9823\n","Epoch [9/10], Step [7500/12500], Loss: 1.0659\n","Epoch [9/10], Step [7600/12500], Loss: 0.6515\n","Epoch [9/10], Step [7700/12500], Loss: 0.1074\n","Epoch [9/10], Step [7800/12500], Loss: 1.1418\n","Epoch [9/10], Step [7900/12500], Loss: 0.2412\n","Epoch [9/10], Step [8000/12500], Loss: 1.7628\n","Epoch [9/10], Step [8100/12500], Loss: 0.8070\n","Epoch [9/10], Step [8200/12500], Loss: 0.0158\n","Epoch [9/10], Step [8300/12500], Loss: 0.7255\n","Epoch [9/10], Step [8400/12500], Loss: 0.2409\n","Epoch [9/10], Step [8500/12500], Loss: 0.5113\n","Epoch [9/10], Step [8600/12500], Loss: 0.0421\n","Epoch [9/10], Step [8700/12500], Loss: 0.8654\n","Epoch [9/10], Step [8800/12500], Loss: 0.9995\n","Epoch [9/10], Step [8900/12500], Loss: 0.6554\n","Epoch [9/10], Step [9000/12500], Loss: 0.6071\n","Epoch [9/10], Step [9100/12500], Loss: 0.0352\n","Epoch [9/10], Step [9200/12500], Loss: 0.7849\n","Epoch [9/10], Step [9300/12500], Loss: 3.4144\n","Epoch [9/10], Step [9400/12500], Loss: 0.1658\n","Epoch [9/10], Step [9500/12500], Loss: 0.0189\n","Epoch [9/10], Step [9600/12500], Loss: 0.2315\n","Epoch [9/10], Step [9700/12500], Loss: 0.5626\n","Epoch [9/10], Step [9800/12500], Loss: 0.0613\n","Epoch [9/10], Step [9900/12500], Loss: 1.0195\n","Epoch [9/10], Step [10000/12500], Loss: 0.2555\n","Epoch [9/10], Step [10100/12500], Loss: 0.6196\n","Epoch [9/10], Step [10200/12500], Loss: 1.2143\n","Epoch [9/10], Step [10300/12500], Loss: 0.8157\n","Epoch [9/10], Step [10400/12500], Loss: 0.7450\n","Epoch [9/10], Step [10500/12500], Loss: 0.3374\n","Epoch [9/10], Step [10600/12500], Loss: 0.8208\n","Epoch [9/10], Step [10700/12500], Loss: 0.1150\n","Epoch [9/10], Step [10800/12500], Loss: 0.1185\n","Epoch [9/10], Step [10900/12500], Loss: 0.6890\n","Epoch [9/10], Step [11000/12500], Loss: 0.6398\n","Epoch [9/10], Step [11100/12500], Loss: 0.7589\n","Epoch [9/10], Step [11200/12500], Loss: 0.8785\n","Epoch [9/10], Step [11300/12500], Loss: 0.7374\n","Epoch [9/10], Step [11400/12500], Loss: 0.3354\n","Epoch [9/10], Step [11500/12500], Loss: 0.3851\n","Epoch [9/10], Step [11600/12500], Loss: 0.5552\n","Epoch [9/10], Step [11700/12500], Loss: 0.4221\n","Epoch [9/10], Step [11800/12500], Loss: 0.5345\n","Epoch [9/10], Step [11900/12500], Loss: 0.5355\n","Epoch [9/10], Step [12000/12500], Loss: 0.8529\n","Epoch [9/10], Step [12100/12500], Loss: 0.4264\n","Epoch [9/10], Step [12200/12500], Loss: 0.2105\n","Epoch [9/10], Step [12300/12500], Loss: 1.0752\n","Epoch [9/10], Step [12400/12500], Loss: 0.4444\n","Epoch [9/10], Step [12500/12500], Loss: 0.5407\n","Epoch [10/10], Step [100/12500], Loss: 0.6634\n","Epoch [10/10], Step [200/12500], Loss: 1.0592\n","Epoch [10/10], Step [300/12500], Loss: 0.7519\n","Epoch [10/10], Step [400/12500], Loss: 0.8596\n","Epoch [10/10], Step [500/12500], Loss: 1.1340\n","Epoch [10/10], Step [600/12500], Loss: 1.1576\n","Epoch [10/10], Step [700/12500], Loss: 0.3058\n","Epoch [10/10], Step [800/12500], Loss: 0.9136\n","Epoch [10/10], Step [900/12500], Loss: 0.0326\n","Epoch [10/10], Step [1000/12500], Loss: 2.0445\n","Epoch [10/10], Step [1100/12500], Loss: 2.2011\n","Epoch [10/10], Step [1200/12500], Loss: 0.9864\n","Epoch [10/10], Step [1300/12500], Loss: 0.9347\n","Epoch [10/10], Step [1400/12500], Loss: 0.2430\n","Epoch [10/10], Step [1500/12500], Loss: 0.5269\n","Epoch [10/10], Step [1600/12500], Loss: 1.6222\n","Epoch [10/10], Step [1700/12500], Loss: 0.4303\n","Epoch [10/10], Step [1800/12500], Loss: 0.7112\n","Epoch [10/10], Step [1900/12500], Loss: 2.5725\n","Epoch [10/10], Step [2000/12500], Loss: 0.0732\n","Epoch [10/10], Step [2100/12500], Loss: 1.8088\n","Epoch [10/10], Step [2200/12500], Loss: 0.1859\n","Epoch [10/10], Step [2300/12500], Loss: 0.6569\n","Epoch [10/10], Step [2400/12500], Loss: 0.1875\n","Epoch [10/10], Step [2500/12500], Loss: 0.3999\n","Epoch [10/10], Step [2600/12500], Loss: 0.5009\n","Epoch [10/10], Step [2700/12500], Loss: 1.1860\n","Epoch [10/10], Step [2800/12500], Loss: 0.8863\n","Epoch [10/10], Step [2900/12500], Loss: 0.7677\n","Epoch [10/10], Step [3000/12500], Loss: 0.1550\n","Epoch [10/10], Step [3100/12500], Loss: 0.8299\n","Epoch [10/10], Step [3200/12500], Loss: 0.4215\n","Epoch [10/10], Step [3300/12500], Loss: 0.1307\n","Epoch [10/10], Step [3400/12500], Loss: 1.3696\n","Epoch [10/10], Step [3500/12500], Loss: 0.7466\n","Epoch [10/10], Step [3600/12500], Loss: 0.5886\n","Epoch [10/10], Step [3700/12500], Loss: 0.2613\n","Epoch [10/10], Step [3800/12500], Loss: 0.7175\n","Epoch [10/10], Step [3900/12500], Loss: 2.1760\n","Epoch [10/10], Step [4000/12500], Loss: 1.8146\n","Epoch [10/10], Step [4100/12500], Loss: 1.2055\n","Epoch [10/10], Step [4200/12500], Loss: 0.8538\n","Epoch [10/10], Step [4300/12500], Loss: 1.1329\n","Epoch [10/10], Step [4400/12500], Loss: 1.7948\n","Epoch [10/10], Step [4500/12500], Loss: 0.0609\n","Epoch [10/10], Step [4600/12500], Loss: 1.0956\n","Epoch [10/10], Step [4700/12500], Loss: 0.2943\n","Epoch [10/10], Step [4800/12500], Loss: 0.3167\n","Epoch [10/10], Step [4900/12500], Loss: 0.6570\n","Epoch [10/10], Step [5000/12500], Loss: 1.3379\n","Epoch [10/10], Step [5100/12500], Loss: 0.2340\n","Epoch [10/10], Step [5200/12500], Loss: 1.0203\n","Epoch [10/10], Step [5300/12500], Loss: 0.5779\n","Epoch [10/10], Step [5400/12500], Loss: 1.0557\n","Epoch [10/10], Step [5500/12500], Loss: 0.6147\n","Epoch [10/10], Step [5600/12500], Loss: 1.6448\n","Epoch [10/10], Step [5700/12500], Loss: 0.5195\n","Epoch [10/10], Step [5800/12500], Loss: 2.3831\n","Epoch [10/10], Step [5900/12500], Loss: 0.4567\n","Epoch [10/10], Step [6000/12500], Loss: 1.2988\n","Epoch [10/10], Step [6100/12500], Loss: 0.3400\n","Epoch [10/10], Step [6200/12500], Loss: 0.9471\n","Epoch [10/10], Step [6300/12500], Loss: 0.9170\n","Epoch [10/10], Step [6400/12500], Loss: 0.7222\n","Epoch [10/10], Step [6500/12500], Loss: 0.6158\n","Epoch [10/10], Step [6600/12500], Loss: 0.9512\n","Epoch [10/10], Step [6700/12500], Loss: 0.6215\n","Epoch [10/10], Step [6800/12500], Loss: 1.1061\n","Epoch [10/10], Step [6900/12500], Loss: 0.2562\n","Epoch [10/10], Step [7000/12500], Loss: 0.5875\n","Epoch [10/10], Step [7100/12500], Loss: 0.5406\n","Epoch [10/10], Step [7200/12500], Loss: 0.7903\n","Epoch [10/10], Step [7300/12500], Loss: 1.0846\n","Epoch [10/10], Step [7400/12500], Loss: 0.4155\n","Epoch [10/10], Step [7500/12500], Loss: 0.2600\n","Epoch [10/10], Step [7600/12500], Loss: 1.0347\n","Epoch [10/10], Step [7700/12500], Loss: 0.6027\n","Epoch [10/10], Step [7800/12500], Loss: 0.0304\n","Epoch [10/10], Step [7900/12500], Loss: 0.7946\n","Epoch [10/10], Step [8000/12500], Loss: 0.5172\n","Epoch [10/10], Step [8100/12500], Loss: 1.1979\n","Epoch [10/10], Step [8200/12500], Loss: 1.0356\n","Epoch [10/10], Step [8300/12500], Loss: 1.7190\n","Epoch [10/10], Step [8400/12500], Loss: 0.1224\n","Epoch [10/10], Step [8500/12500], Loss: 1.3453\n","Epoch [10/10], Step [8600/12500], Loss: 0.1784\n","Epoch [10/10], Step [8700/12500], Loss: 2.7436\n","Epoch [10/10], Step [8800/12500], Loss: 0.9073\n","Epoch [10/10], Step [8900/12500], Loss: 0.9745\n","Epoch [10/10], Step [9000/12500], Loss: 0.4781\n","Epoch [10/10], Step [9100/12500], Loss: 0.5565\n","Epoch [10/10], Step [9200/12500], Loss: 0.4613\n","Epoch [10/10], Step [9300/12500], Loss: 0.0413\n","Epoch [10/10], Step [9400/12500], Loss: 1.2228\n","Epoch [10/10], Step [9500/12500], Loss: 0.2320\n","Epoch [10/10], Step [9600/12500], Loss: 0.1258\n","Epoch [10/10], Step [9700/12500], Loss: 0.1467\n","Epoch [10/10], Step [9800/12500], Loss: 1.5564\n","Epoch [10/10], Step [9900/12500], Loss: 0.1588\n","Epoch [10/10], Step [10000/12500], Loss: 0.8043\n","Epoch [10/10], Step [10100/12500], Loss: 0.2600\n","Epoch [10/10], Step [10200/12500], Loss: 0.7245\n","Epoch [10/10], Step [10300/12500], Loss: 0.2678\n","Epoch [10/10], Step [10400/12500], Loss: 0.8479\n","Epoch [10/10], Step [10500/12500], Loss: 0.9541\n","Epoch [10/10], Step [10600/12500], Loss: 1.5041\n","Epoch [10/10], Step [10700/12500], Loss: 0.1609\n","Epoch [10/10], Step [10800/12500], Loss: 0.3711\n","Epoch [10/10], Step [10900/12500], Loss: 0.5564\n","Epoch [10/10], Step [11000/12500], Loss: 0.2633\n","Epoch [10/10], Step [11100/12500], Loss: 0.9098\n","Epoch [10/10], Step [11200/12500], Loss: 1.1615\n","Epoch [10/10], Step [11300/12500], Loss: 0.1227\n","Epoch [10/10], Step [11400/12500], Loss: 0.0492\n","Epoch [10/10], Step [11500/12500], Loss: 1.0964\n","Epoch [10/10], Step [11600/12500], Loss: 1.1449\n","Epoch [10/10], Step [11700/12500], Loss: 0.9992\n","Epoch [10/10], Step [11800/12500], Loss: 1.2265\n","Epoch [10/10], Step [11900/12500], Loss: 2.6562\n","Epoch [10/10], Step [12000/12500], Loss: 0.9292\n","Epoch [10/10], Step [12100/12500], Loss: 0.2834\n","Epoch [10/10], Step [12200/12500], Loss: 0.3812\n","Epoch [10/10], Step [12300/12500], Loss: 0.4808\n","Epoch [10/10], Step [12400/12500], Loss: 0.3420\n","Epoch [10/10], Step [12500/12500], Loss: 0.1528\n","Accuracy on CIFAR-10 test images: 76.62%\n"]}]}]}