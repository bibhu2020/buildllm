{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNS133CFj0NIcK9yME1EYkk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Softmax and Cross-Entropy\n","Softmax and Cross-Entropy are mostly used functions in the neural network.\n"],"metadata":{"id":"hgLfGfTAde6s"}},{"cell_type":"markdown","source":["# üßÆ Softmax Function\n","\n","The **Softmax** function converts a vector of real numbers into a **probability distribution**, where each value is between **0 and 1**, and all probabilities **sum to 1**.\n","\n","![softmax](https://media.geeksforgeeks.org/wp-content/uploads/20240706012340/Softmax-Activation-Function.webp)\n","\n","---\n","\n","## üîπ Definition\n","\n","Given an input vector:\n","\n","$$\n","z = [z_1, z_2, ..., z_n]\n","$$\n","\n","The **Softmax** function is defined as:\n","\n","$$\n","\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}} \\quad \\text{for } i = 1, 2, ..., n\n","$$\n","\n","---\n","\n","## What's the use?\n","- The **Softmax** function is mainly used in **multi-class classification problems** ‚Äî i.e., when your model needs to predict one class out of many possible classes (e.g., recognizing digits 0‚Äì9 or categories like cat/dog/horse).\n","\n","- Neural networks usually output arbitrary real numbers, called logits. Softmax transforms these logits into probabilities that are between 0 to 1. And they all sum up to 1. **The largest output gets the highest probability.**\n","\n","- Without **softmax**, the logits will be unbounded. The output will be of any number. Hence, tagging it to a label becomes difficult.\n"],"metadata":{"id":"cwg413VreNPy"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IsyDr-uhda4_","executionInfo":{"status":"ok","timestamp":1762432427314,"user_tz":480,"elapsed":27,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"568a7734-bf3d-4b6b-ad8b-4294701c9e1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["softmax numpy: [0.65900114 0.24243297 0.09856589]\n","softmax torch: tensor([0.6590, 0.2424, 0.0986])\n"]}],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","# ---------- NumPy Implementation ----------\n","\n","# Define the softmax function manually using NumPy\n","def softmax(x):\n","    # Exponentiate each value and divide by the sum of all exponentials\n","    # This converts raw scores (logits) into probabilities that sum to 1\n","    return np.exp(x) / np.sum(np.exp(x), axis=0)\n","\n","# Input vector (logits) ‚Äî raw model outputs before normalization\n","x = np.array([2.0, 1.0, 0.1])\n","\n","# Compute softmax probabilities\n","outputs = softmax(x)\n","print('softmax numpy:', outputs)\n","\n","# ---------- PyTorch Implementation ----------\n","\n","# Convert the same input into a PyTorch tensor\n","x = torch.tensor([2.0, 1.0, 0.1])\n","\n","# Use PyTorch's built-in softmax function\n","# dim=0 means we apply softmax along the first (and only) dimension of the tensor\n","outputs = torch.softmax(x, dim=0)\n","\n","print('softmax torch:', outputs)\n","\n","# ‚úÖ Both implementations should give the same result.\n","# The outputs represent the probabilities of each class,\n","# and they always sum up to 1.\n"]},{"cell_type":"markdown","source":["# üßÆ Sigmoid Function\n","\n","The **Sigmoid** (or **Logistic**) function maps any real-valued number into a value between **0 and 1**.  \n","It is widely used to model probabilities in **binary classification problems**.\n","\n","---\n","\n","## üîπ Definition\n","\n","Given an input \\( z \\in \\mathbb{R} \\):\n","\n","$$\n","\\sigma(z) = \\frac{1}{1 + e^{-z}}\n","$$\n","\n","The output \\( \\sigma(z) \\) is always between **0 and 1**.\n","\n","| z value | Output (œÉ(z)) | Interpretation |\n","|----------|----------------|----------------|\n","| Large positive | ‚Üí 1 | Strong positive class confidence |\n","| Large negative | ‚Üí 0 | Strong negative class confidence |\n","| 0 | 0.5 | Uncertain (equal probability) |\n","\n","---\n","\n","## üîπ Derivative\n","\n","The derivative of the sigmoid function is:\n","\n","$$\n","\\sigma'(z) = \\sigma(z) \\times (1 - \\sigma(z))\n","$$\n","\n","This property makes it smooth and differentiable ‚Äî ideal for gradient-based optimization during neural network training.\n","\n","---\n","\n","## üîπ What's the use?\n","\n","- The **Sigmoid** function is primarily used in **binary classification** problems where the output represents the probability of belonging to the **positive class** (e.g., ‚Äúyes/no‚Äù, ‚Äúspam/not spam‚Äù).  \n","\n","- It‚Äôs also used in:\n","  - The **output layer** of binary classifiers.  \n","  - **Gating mechanisms** in RNNs and LSTMs (to control information flow).  \n","  - Logistic regression to model probabilities.\n","\n","---\n","\n","## üîπ Example\n","\n","For a model output (logit):\n","\n","$$\n","z = 2.0\n","$$\n","\n","The sigmoid activation is:\n","\n","$$\n","\\sigma(2.0) = \\frac{1}{1 + e^{-2.0}} \\approx 0.88\n","$$\n","\n","This means the model is **88% confident** in predicting the positive class.\n","\n","---\n","\n","## üß† Intuition\n","\n","- When \\( z \\) is large and positive ‚Üí output ‚âà **1**  \n","- When \\( z \\) is large and negative ‚Üí output ‚âà **0**  \n","- It **squashes** unbounded input values into a small, smooth range \\([0, 1]\\), making it perfect for representing probabilities.\n","\n","---\n","\n","‚úÖ **In summary:**\n","- Use **Sigmoid** for **binary classification**.  \n","- Use **Softmax** for **multi-class classification** (one of many classes).  \n"],"metadata":{"id":"nmtjQiKCJB3G"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","# ---------- NumPy Implementation ----------\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","x = np.array([2.0, 1.0, 0.1])\n","outputs = sigmoid(x)\n","print('sigmoid numpy:', outputs)\n","\n","# ---------- PyTorch Implementation ----------\n","x = torch.tensor([2.0, 1.0, 0.1])\n","outputs = torch.sigmoid(x)\n","print('sigmoid torch:', outputs)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hvUDEzqwJsHp","executionInfo":{"status":"ok","timestamp":1762432316862,"user_tz":480,"elapsed":7558,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"f6d3a81d-284f-4cef-d82e-08b89326b74d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["sigmoid numpy: [0.88079708 0.73105858 0.52497919]\n","sigmoid torch: tensor([0.8808, 0.7311, 0.5250])\n"]}]},{"cell_type":"markdown","source":["# üßÆ Cross-Entropy Loss  \n","\n","**Cross-Entropy Loss** (also called **Log Loss**) measures the difference between two probability distributions ‚Äî the **true labels** (what the data actually is) and the **predicted probabilities** (what your model outputs).\n","\n","\n","It is the most common loss function for classification problems, especially when used in conjunction with a **softmax** output layer.\n","\n","In a multi-class classification setting, we usually one-hot encode the true label.  \n","\n","For example:  \n","$$\n","y_{\\text{true}} = [1, 0, 0, 0]\n","$$\n","\n","This indicates the correct class is the first one.  \n","Suppose the model‚Äôs softmax output is:\n","\n","$$\n","y_{\\text{hat}} = [0.4, 0.1, 0.3, 0.2]\n","$$\n","\n","This means the model predicts the first class is most likely, but only with **40% confidence**.  \n","Cross-entropy loss quantifies how ‚Äúwrong‚Äù or ‚Äúuncertain‚Äù that prediction is ‚Äî the training objective is to **minimize** this loss.\n","\n","---\n","\n","## üîπ Formulae  \n","\n","### ‚úÖ Multi-Class Cross-Entropy (Single Sample)\n","\n","$$\n","L = - \\sum_{j=1}^{C} y_{j} \\log(\\hat{y}_{j})\n","$$\n","\n","where:  \n","- \\(C\\) = number of classes  \n","- \\(y_{j}\\) = true label for class *j* (one-hot: 1 for correct class, 0 for others)  \n","- \\(\\hat{y}_{j}\\) = predicted probability that the sample belongs to class *j*  \n","\n","---\n","\n","### ‚úÖ Multi-Class Cross-Entropy (Batch)\n","\n","$$\n","\\text{Loss}_{\\text{batch}} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{i,j} \\log(\\hat{y}_{i,j})\n","$$\n","\n","where \\(N\\) is the number of samples.\n","\n","---\n","\n","### ‚úÖ Binary Cross-Entropy\n","\n","$$\n","L = -\\frac{1}{N} \\sum_{i=1}^{N} \\Big( y_{i} \\log(p_{i}) + (1 - y_{i}) \\log(1 - p_{i}) \\Big)\n","$$\n","\n","where:  \n","- $y_{i} \\in \\{0,1\\}$ is the true label for sample *i*  \n","- $p_{i}$ is the predicted probability of class ‚Äú1‚Äù for sample *i*\n","\n","---\n","\n","## üîπ Worked Example  \n","\n","Using the earlier example:  \n","\n","- True label:  \n","  $y_{\\text{true}} = [1, 0, 0, 0]$  \n","- Predicted probabilities:  \n","  $\\hat{y} = [0.4, 0.1, 0.3, 0.2]$\n","\n","Then the loss is:\n","\n","$$\n","\\begin{aligned}\n","L &= - \\Big(1 \\times \\log(0.4) + 0 \\times \\log(0.1) + 0 \\times \\log(0.3) + 0 \\times \\log(0.2)\\Big) \\\\\n","  &= -\\log(0.4) \\\\\n","  &\\approx -(-0.916) \\quad (\\text{since } \\log(0.4) \\approx -0.916) \\\\\n","  &\\approx 0.916\n","\\end{aligned}\n","$$\n","\n","So the **cross-entropy loss ‚âà 0.916**.  \n","That‚Äôs fairly high (since perfect confidence would give loss ‚Üí 0). It indicates the model is uncertain in its prediction (40% confidence) and there‚Äôs room for improvement.\n","\n","If instead the model predicted:  \n","\n","$$\n","\\hat{y} = [0.9, 0.05, 0.03, 0.02]\n","$$\n","\n","Then:\n","\n","$$\n","L = -\\log(0.9) \\approx 0.105\n","$$\n","\n","Which is much lower ‚Äî showing the model is more confident and the loss is smaller.\n","\n","---\n","\n","## üß† Intuition & Key Points  \n","\n","- The lower the loss, the **closer** the predicted distribution is to the true distribution (i.e., high probability on the true class).\n","\n","- If the model gives very low probability to the correct class (e.g., 0.1), the loss becomes large ‚Äî penalizing confident but wrong predictions heavily.\n","\n","- This loss function **works smoothly** with gradient-based optimization because it‚Äôs differentiable and provides strong gradients when predictions are poor.\n","\n","- Typically used with **softmax** in multi-class classification, or with **sigmoid** in binary classification.\n"],"metadata":{"id":"_LRzXuzaGcJ-"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# ---------- Example 1: Sigmoid output (Binary Classification) ----------\n","\n","# Logits from the model (before sigmoid)\n","logits = torch.tensor([2.0, -1.0, 0.5]) # 3 samples\n","\n","# True binary labels\n","y_true = torch.tensor([1.0, 0.0, 1.0]) # 3 samples\n","\n","# Sigmoid probabilities\n","y_pred_sigmoid = torch.sigmoid(logits)\n","print(\"Sigmoid outputs:\", y_pred_sigmoid)\n","\n","# Binary Cross-Entropy Loss (using BCELoss)\n","bce_loss = nn.BCELoss()\n","loss_sigmoid = bce_loss(y_pred_sigmoid, y_true)\n","print(\"Binary Cross-Entropy Loss (Sigmoid):\", loss_sigmoid.item())\n","\n","# ---------- Example 2: Softmax output (Multi-Class Classification) ----------\n","\n","# Logits for 3 samples and 4 classes (before softmax)\n","logits_multi = torch.tensor([[2.0, 1.0, 0.1, 0.5],\n","                             [0.3, 2.2, 0.5, 1.1],\n","                             [1.2, 0.7, 1.5, 0.3]])\n","\n","# True class labels (indices of correct class)\n","y_true_multi = torch.tensor([0, 1, 2])  # class indices\n","\n","# Softmax probabilities (optional, not needed for CrossEntropyLoss)\n","y_pred_softmax = torch.softmax(logits_multi, dim=1)\n","print(\"\\nSoftmax outputs:\", y_pred_softmax)\n","\n","# Categorical Cross-Entropy Loss (using CrossEntropyLoss)\n","# Note: CrossEntropyLoss in PyTorch expects raw logits (not softmax)\n","ce_loss = nn.CrossEntropyLoss()\n","loss_softmax = ce_loss(logits_multi, y_true_multi)\n","print(\"Cross-Entropy Loss (Softmax):\", loss_softmax.item())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oGyAXb0LKyMg","executionInfo":{"status":"ok","timestamp":1762432686376,"user_tz":480,"elapsed":24,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"2f2bea00-9212-4501-c285-56c00c1e7deb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Sigmoid outputs: tensor([0.8808, 0.2689, 0.6225])\n","Binary Cross-Entropy Loss (Sigmoid): 0.3047555685043335\n","\n","Softmax outputs: tensor([[0.5745, 0.2114, 0.0859, 0.1282],\n","        [0.0898, 0.6006, 0.1097, 0.1999],\n","        [0.2974, 0.1804, 0.4014, 0.1209]])\n","Cross-Entropy Loss (Softmax): 0.6589792370796204\n"]}]},{"cell_type":"markdown","source":["# A Practical Example\n","Imagine we have a neural network that predicts which fruit is in an image.\n","\n","**Classes**: 0 = Apple (class0), 1 = Banana (class1), 2 = Cherry (class2)\n","\n","3 images (samples) we want to classify.\n","\n","\n"],"metadata":{"id":"YaBW3I20Y3Yx"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","# CrossEntropyLoss combines Softmax + Log Loss internally.\n","loss = nn.CrossEntropyLoss()\n","\n","# 3 samples images that we are passing through the neurtal network to classify.\n","#                    [Sample1, Sample2, Sample3]\n","#                    [Cherry, Apple, Banana]\n","Y_true =torch.tensor([2,0,1])\n","\n","# n_samples x n_classes = 3 x 3\n","# Good predictions (logits)\n","# It is using one-hot encoding to represent the output from neural network\n","#                           [class0, class1, class2]\n","Y_pred_good = torch.tensor([[0.1,1.0,2.1],   # Sample 1: highest score for class 2 ‚Üí correct\n","                            [2.0,1.0,0.1],   # Sample 2: highest score for class 0 ‚Üí correct\n","                            [0.1,3.0,0.1]])  # Sample 3: highest score for class 1 ‚Üí correct\n","\n","# Bad predictions (logits)\n","Y_pred_bad = torch.tensor([ [2.1,1.0,0.1],   # Sample 1: highest score for class 0 ‚Üí wrong\n","                            [0.1,1.0,2.1],   # Sample 2: highest score for class 2 ‚Üí wrong\n","                            [0.1,3.0, 0.1]]) # Sample 3: highest score for class 1 ‚Üí correct\n","\n","# CrossEntropyLoss combines Softmax + Log Loss internally.\n","l1 = loss(Y_pred_good, Y_true)\n","l2 = loss(Y_pred_bad, Y_true)\n","\n","print(f'Loss1 numpy: {l1.item():.4f}') # small loss ‚Üí good predictions\n","print(f'Loss2 numpy: {l2.item():.4f}') # larger loss ‚Üí bad predictions\n","\n","# Get predicted classes\n","_, predictions1 = torch.max(Y_pred_good, 1)\n","_, predictions2 = torch.max(Y_pred_bad, 1)\n","\n","print(predictions1) # tensor([2, 0, 1]) ‚Üí matches with true labels y_true\n","print(predictions2) # tensor([0, 2, 1]) ‚Üí some wrong predictions\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKjDN07uW4yZ","executionInfo":{"status":"ok","timestamp":1762437184434,"user_tz":480,"elapsed":14,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"7c3a5eb5-1536-44ec-8564-d887b099ac03"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss1 numpy: 0.3018\n","Loss2 numpy: 1.6242\n","tensor([2, 0, 1])\n","tensor([0, 2, 1])\n"]}]},{"cell_type":"markdown","source":["# A Neural Network for Binary Classification (using Cross Entropy Loss)\n"],"metadata":{"id":"upjLgeIGcrJX"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# ----------------------------\n","# 1Ô∏è‚É£ Define NeuralNet1\n","# ----------------------------\n","class NeuralNet1(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(NeuralNet1, self).__init__()\n","        self.linear1 = nn.Linear(input_size, hidden_size)  # Input layer\n","        self.relu = nn.ReLU()                              # Activation\n","        self.linear2 = nn.Linear(hidden_size, 1)          # Output layer\n","\n","    def forward(self, x):\n","        out = self.linear1(x)\n","        out = self.relu(out)\n","        out = self.linear2(out)\n","        y_pred = torch.sigmoid(out)  # sigmoid for binary classification\n","        return y_pred\n","\n","# ----------------------------\n","# 2Ô∏è‚É£ Prepare Binary Classification Data\n","# ----------------------------\n","X_numpy, y_numpy = make_classification(\n","    n_samples=200, n_features=4, n_informative=2, n_redundant=0, n_classes=2, random_state=42\n",")\n","\n","# Scale features\n","scaler = StandardScaler()\n","X_numpy = scaler.fit_transform(X_numpy)\n","\n","# Convert to tensors\n","X = torch.tensor(X_numpy, dtype=torch.float32)\n","y = torch.tensor(y_numpy.reshape(-1, 1), dtype=torch.float32)\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# ----------------------------\n","# 3Ô∏è‚É£ Initialize Model, Loss, Optimizer\n","# ----------------------------\n","input_size = X.shape[1]\n","hidden_size = 5\n","model = NeuralNet1(input_size=input_size, hidden_size=hidden_size)\n","\n","criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","\n","# ----------------------------\n","# 4Ô∏è‚É£ Training Loop\n","# ----------------------------\n","epochs = 100\n","for epoch in range(epochs):\n","    y_pred = model(X_train)\n","    loss = criterion(y_pred, y_train)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch+1:03d}, Loss: {loss.item():.4f}\")\n","\n","# ----------------------------\n","# 5Ô∏è‚É£ Evaluate on Test Set\n","# ----------------------------\n","with torch.no_grad():\n","    y_test_pred = model(X_test)\n","    y_test_pred_class = (y_test_pred > 0.5).float()\n","    accuracy = (y_test_pred_class == y_test).sum() / y_test.shape[0]\n","    print(f\"NeuralNet1 Test Accuracy: {accuracy.item() * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Wot1CtLcwgh","executionInfo":{"status":"ok","timestamp":1762438201836,"user_tz":480,"elapsed":10007,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"79439e12-562c-40c3-b7ba-992bbfc4258a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 001, Loss: 0.7370\n","Epoch 011, Loss: 0.6855\n","Epoch 021, Loss: 0.6445\n","Epoch 031, Loss: 0.6041\n","Epoch 041, Loss: 0.5618\n","Epoch 051, Loss: 0.5207\n","Epoch 061, Loss: 0.4832\n","Epoch 071, Loss: 0.4518\n","Epoch 081, Loss: 0.4273\n","Epoch 091, Loss: 0.4082\n","NeuralNet1 Test Accuracy: 82.50%\n"]}]},{"cell_type":"markdown","source":["# A Neural Network for Multi Classification (using Cross Entropy Loss)"],"metadata":{"id":"XdfEQpo8gKWI"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# ----------------------------\n","# 1Ô∏è‚É£ Define NeuralNet2\n","# ----------------------------\n","class NeuralNet2(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(NeuralNet2, self).__init__()\n","        self.linear1 = nn.Linear(input_size, hidden_size)    # Input layer\n","        self.relu = nn.ReLU()                                # Activation\n","        self.linear2 = nn.Linear(hidden_size, num_classes)  # Output layer\n","\n","    def forward(self, x):\n","        out = self.linear1(x)\n","        out = self.relu(out)\n","        out = self.linear2(out)  # raw logits\n","        return out\n","\n","# ----------------------------\n","# 2Ô∏è‚É£ Prepare Multi-Class Data\n","# ----------------------------\n","X_numpy, y_numpy = make_classification(\n","    n_samples=300, n_features=4, n_informative=3, n_redundant=0, n_classes=3, random_state=42\n",")\n","\n","# Scale features\n","scaler = StandardScaler()\n","X_numpy = scaler.fit_transform(X_numpy)\n","\n","# Convert to tensors\n","X = torch.tensor(X_numpy, dtype=torch.float32)\n","y = torch.tensor(y_numpy, dtype=torch.long)  # integer labels required for CrossEntropyLoss\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# ----------------------------\n","# 3Ô∏è‚É£ Initialize Model, Loss, Optimizer\n","# ----------------------------\n","input_size = X.shape[1]\n","hidden_size = 5\n","num_classes = 3\n","\n","model = NeuralNet2(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\n","\n","criterion = nn.CrossEntropyLoss()  # Multi-class loss\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","\n","# ----------------------------\n","# 4Ô∏è‚É£ Training Loop\n","# ----------------------------\n","epochs = 100\n","for epoch in range(epochs):\n","    logits = model(X_train)\n","    loss = criterion(logits, y_train)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch+1:03d}, Loss: {loss.item():.4f}\")\n","\n","# ----------------------------\n","# 5Ô∏è‚É£ Evaluate on Test Set\n","# ----------------------------\n","with torch.no_grad():\n","    logits_test = model(X_test)\n","    y_test_pred = torch.argmax(logits_test, dim=1)\n","    accuracy = (y_test_pred == y_test).sum() / y_test.shape[0]\n","    print(f\"NeuralNet2 Test Accuracy: {accuracy.item() * 100:.2f}%\")\n"],"metadata":{"id":"dCURabjegMFM","executionInfo":{"status":"ok","timestamp":1762438219569,"user_tz":480,"elapsed":100,"user":{"displayName":"Bibhu Mishra","userId":"12646824449670614646"}},"outputId":"30446044-5f2b-4470-c6c1-3e3b39493418","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 001, Loss: 1.1683\n","Epoch 011, Loss: 1.1031\n","Epoch 021, Loss: 1.0626\n","Epoch 031, Loss: 1.0289\n","Epoch 041, Loss: 0.9984\n","Epoch 051, Loss: 0.9686\n","Epoch 061, Loss: 0.9369\n","Epoch 071, Loss: 0.9056\n","Epoch 081, Loss: 0.8763\n","Epoch 091, Loss: 0.8489\n","NeuralNet2 Test Accuracy: 63.33%\n"]}]}]}