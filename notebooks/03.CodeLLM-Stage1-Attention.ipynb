{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc8e8ee",
   "metadata": {},
   "source": [
    "## Attention Mechanisms\n",
    "\n",
    "![Attention](https://camo.githubusercontent.com/966b15715dd8d9cf0c1eda75a5e84c06f172e505e2ad745871c97b2af0abecd3/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830335f636f6d707265737365642f30312e776562703f313233)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb295e4",
   "metadata": {},
   "source": [
    "In the early phase of NLU and NLG, the neural network was in use were RNN, LSTM, and GRU. \n",
    "\n",
    "The Problem with RNNs and LSTMs/GRU\n",
    "\n",
    "- **Sequential bottleneck**: RNNs/LSTMs read one token at a time â†’ slow, hard to parallelize.  \n",
    "\n",
    "- **Long-term dependencies**: LSTM retain information over longer sequences than RNN. Howerver, both LSTMs & RNNs compress all past information into a single hidden vector. A single vector was unable to store all nuances leading to information loss about distant or multiple relevant tokens.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
