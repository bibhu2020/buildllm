{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80261d12",
   "metadata": {},
   "source": [
    "# LoRA (Low Rank Adaptation)\n",
    "LoRA is Parameter Efficient Fine Tunning (PEFT). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03d6c1b",
   "metadata": {},
   "source": [
    "## How LoRA works:\n",
    "\n",
    "- Freeze your original weights and parameters of the base model. the original matrix dimension is d x d.(w --> represents the original frozen weights). \n",
    "\n",
    "- Ingest 2 rank decomposition matrix. One matrix is of d x r dimension. the other is r x d dimension. (**here is the r is the rank**). Typically you keep r low. It means you create a small size matrix to train in order to lower the resource need.\n",
    "\n",
    "- Train the weights of the 2 matrix only created in the above step. \n",
    "\n",
    "- Multiply the 2 matrix is produce d x d trained weight matrix. (dW --> newly introduced trained weights)\n",
    "\n",
    "- Update the original model matrix. (w + dw)\n",
    "\n",
    "![lora](https://towardsdatascience.com/wp-content/uploads/2023/12/1Vp90_0_4B3eOGuzU8lF-dg.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
