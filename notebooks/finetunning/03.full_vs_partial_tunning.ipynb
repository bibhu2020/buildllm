{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3bd444",
   "metadata": {},
   "source": [
    "# Full Fine Tunning vs Partial Fine Tunning\n",
    "\n",
    "**Full Fine Tunning:**\n",
    "- All parameters are updated.\n",
    "- Cost of training is high.\n",
    "- Cost of storage is also high.\n",
    "- model overfitting issue.\n",
    "- fine-tunned model size is same base model.\n",
    "- less scalable (because all original parameters are altered). It means, it can't be used for multi-purposes.\n",
    "- Used for very specific usecase and you do not bother about resource cost.\n",
    "\n",
    "**Partial Fine Tunning:**\n",
    "- Only a small set of parameters are updated.\n",
    "- less training cost\n",
    "- less storage required.\n",
    "- no or less overfitting issue.\n",
    "- size is almost same as the base model. Because a large portion of the original parameters are untoched, it can be used for multi-purposes still.\n",
    "- It is resource friendly and the tunned model is still generic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337aa3df",
   "metadata": {},
   "source": [
    "# Ways to Perform Partial Fine Tunning\n",
    "\n",
    "- **Parameter Efficient Fine Tunning (PEFT)**: Fine tunning a small subset of the model parameters. Technique used is LORA and QLORA.\n",
    "\n",
    "- **Feature-Based Fine Tunning**: Use a pre-trained model as feature extractor. \n",
    "\n",
    "- **Layer-Wise Fine Tunning**: Train specific layer of neural network. \n",
    "\n",
    "- **Knowledge Distillations**: Transfer knowledge from large model to small model.\n",
    "\n",
    "- **Meta Learning**: Learn to adapot new task with minimal updates.\n",
    "\n",
    "- **Selective Fine Tunning**: \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
