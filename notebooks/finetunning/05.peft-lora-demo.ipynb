{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1226e0",
   "metadata": {},
   "source": [
    "# LoRA Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227f396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "849910cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "# Standard GLUE SST-2 dataset - Sentiment Analysis of given sentences\n",
    "dataset =  load_dataset(\"glue\", \"sst2\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1859a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding PAD token, tokenizer vocalbulary size: 50265\n",
      "Before adding PAD token, tokenizer padding token: <pad>\n",
      "After adding PAD token, tokenizer vocalbulary size: 50265\n",
      "After adding PAD token, tokenizer padding token: <pad>\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"roberta-base\" \n",
    "\n",
    "# define label maps\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "# AutoConfig loads RoBERTa’s default configuration but overrides some fields:\n",
    "# num_labels=2 → adds a classification head with 2 output labels\n",
    "# id2label and label2id → maps between label IDs and label names\n",
    "config = AutoConfig.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Loads tokenizer for RoBERTa\n",
    "# This tokenizer: Splits text into tokens, converts tokens to IDs, and handles special tokens like [CLS], [SEP], and [PAD].\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# RoBERTa itself is not specifically a classification model; it's a general language model.\n",
    "# But when we load it with AutoModelForSequenceClassification, it becomes a classifier.\n",
    "# why? Because we specify the config with num_labels=2\n",
    "# how? By using AutoModelForSequenceClassification, we are telling the model to add a classification head on top of the base RoBERTa model.\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(f\"Before adding PAD token, tokenizer vocalbulary size: {len(tokenizer)}\")\n",
    "print(f\"Before adding PAD token, tokenizer padding token: {tokenizer.pad_token}\")\n",
    "\n",
    "# Padding is needed because transformer models (like RoBERTa, BERT, GPT) \n",
    "# only work with fixed-length batches, but sentences in real life have variable lengths.\n",
    "# RoBERTa does NOT have a pad token by default.\n",
    "# It uses the <mask> token as padding—but this is not ideal for training.\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Adding PAD token to tokenizer and resizing model embeddings...\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"After adding PAD token, tokenizer vocalbulary size: {len(tokenizer)}\")\n",
    "print(f\"After adding PAD token, tokenizer padding token: {tokenizer.pad_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee2899a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): lora.Linear(\n",
      "                (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): ModulesToSaveWrapper(\n",
      "    (original_module): RobertaClassificationHead(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "    (modules_to_save): ModuleDict(\n",
      "      (default): RobertaClassificationHead(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Base Model Structure\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b85c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cafece88309409e8b6ef5a762aff64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tokenization function\n",
    "# this function will be applied to each record in the dataset\n",
    "# it extracts the sentence, tokenizes it to IDs, and truncates/pads to max length of 512\n",
    "def tokenize_function(examples):\n",
    "    # extract the sentence\n",
    "    sentences = examples[\"sentence\"]\n",
    "    # tokenize and truncate/pad to max length\n",
    "    tokenizer.truncation_side = 'left'\n",
    "    tokenized_inputs = tokenizer(\n",
    "        sentences, \n",
    "        return_tensors='np',\n",
    "        truncation=True, \n",
    "        max_length=512 \n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# tokenize training and validation datasets\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c0d03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator to dynamically pad the inputs received, so they are of equal length within a batch\n",
    "# Data collators are used to batch multiple samples of data together and prepare it for training.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a326ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate library (by Hugging Face) lets you load standard evaluation metrics.\n",
    "# it caldculates accuracy by comparing predicted labels to true labels.\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# define evaluation function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e3daf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of samples for testing the tokenizer \n",
    "text_list = [\n",
    "    \"I loved the new Batman movie!\",\n",
    "    \"The food at that restaurant was terrible.\",\n",
    "    \"What an amazing experience!\",\n",
    "    \"I will never go back to that place again.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e9dac",
   "metadata": {},
   "source": [
    "## Test the BASE MODEL (before Finetunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01141bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "Text: I loved the new Batman movie! - POSITIVE\n",
      "\n",
      "Text: The food at that restaurant was terrible. - POSITIVE\n",
      "\n",
      "Text: What an amazing experience! - POSITIVE\n",
      "\n",
      "Text: I will never go back to that place again. - POSITIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Untrained model predictions:\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = base_model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    print(f\"Text: {text} - {id2label[predictions.item()]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec30d64",
   "metadata": {},
   "source": [
    "## Finetunning using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b6dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model size: 125.39 million parameters\n"
     ]
    }
   ],
   "source": [
    "# print base model size\n",
    "base_model_size = sum(param.numel() for param in base_model.parameters())\n",
    "print(f\"Base model size: {base_model_size/1e6:.2f} million parameters\") \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57be36dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 739,586 || all params: 125,386,756 || trainable%: 0.5898\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, # Task type = sequence classification (e.g., sentiment analysis). This tells LoRA which parts of the model to modify.\n",
    "    inference_mode=False, # Set to False because you are training. True would freeze the base model for inference.\n",
    "    r=4, # Rank of the low-rank decomposition. LoRA inserts small weight matrices of size r instead of modifying the full weight matrix.\n",
    "    lora_alpha=32, # Scaling factor for LoRA weights (helps control magnitude).\n",
    "    lora_dropout=0.1, # Dropout applied to LoRA layers during training (prevents overfitting).\n",
    "    target_modules=[\"query\"] # target_modules specifies which parts of the transformer model will get LoRA adapters.\n",
    "                            # In multi-head attention, each attention layer has weights for query (Q), key (K), value (V), and output (O)\n",
    "                            # By setting target_modules=[\"query\"], LoRA will only inject trainable adapters into the query weight matrices.\n",
    "                            # This reduces the number of trainable parameters even further.\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e332b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # peft_model size\n",
    "# peft_model_size = sum(param.numel() for param in peft_model.parameters())\n",
    "# print(f\"PEFT model size: {peft_model_size/1e6:.2f} million parameters\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51fd7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 2e-4\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Training Configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs/peft-lora-sst2\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62192bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_186986/428292865.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/home/azureuser/ws/buildllm/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='698' max='4210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 698/4210 28:28 < 2:23:41, 0.41 it/s, Epoch 0.17/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m trainer = Trainer(\n\u001b[32m      3\u001b[39m     model=peft_model,\n\u001b[32m      4\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/buildllm/.venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/buildllm/.venv/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/buildllm/.venv/lib/python3.12/site-packages/transformers/trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/buildllm/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2852\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2850\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2851\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/buildllm/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/buildllm/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/buildllm/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train the model using Trainer API\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f3bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "# Test the fine-tuned model\n",
    "print(\"Fine-tuned model predictions:\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = peft_model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Text: {text} - {id2label[predictions.item()]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a0fdf1",
   "metadata": {},
   "source": [
    "## Upload the model to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Upload Fine-Tuned Model to Hugging Face Hub ===\n",
    "# Make sure you have:\n",
    "# 1. Installed `huggingface_hub` (`pip install huggingface_hub`)\n",
    "# 2. Logged in using `huggingface-cli login` or set the HF_TOKEN environment variable\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Define repository ID (format: \"username/model_name\") on Hugging Face Hub\n",
    "# Example: \"your-username/roberta-lora-sst2\"\n",
    "repo_id = \"your-username/roberta-lora-sst2\"\n",
    "\n",
    "# Save the LoRA adapter weights only (recommended for LoRA)\n",
    "peft_model.save_pretrained(f\"./outputs/peft-lora-sst2\")\n",
    "\n",
    "# Save tokenizer as well\n",
    "tokenizer.save_pretrained(f\"./outputs/peft-lora-sst2\")\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "# This will upload both the LoRA weights and tokenizer to your repository\n",
    "peft_model.push_to_hub(repo_id, use_auth_token=True)\n",
    "tokenizer.push_to_hub(repo_id, use_auth_token=True)\n",
    "\n",
    "print(f\"Fine-tuned LoRA model and tokenizer successfully uploaded to Hugging Face Hub: https://huggingface.co/{repo_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60044d",
   "metadata": {},
   "source": [
    "## Upload the MODEL card to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN_WRITE\")\n",
    "\n",
    "readme_content = \"\"\"\n",
    "---\n",
    "license: mit\n",
    "tags:\n",
    "  - causal-lm\n",
    "  - instruction-following\n",
    "  - loRA\n",
    "  - QLoRA\n",
    "  - quantized\n",
    "language: en\n",
    "library_name: transformers\n",
    "base_model: microsoft/phi-2\n",
    "---\n",
    "\n",
    "# Phi-2 QLoRA Fine-Tuned Model\n",
    "\n",
    "\n",
    "**Model:** `mishrabp/phi2-qlora-finetuned`\n",
    "\n",
    "**Base Model:** [`microsoft/phi-2`](https://huggingface.co/microsoft/phi-2)\n",
    "\n",
    "**Fine-Tuning Method:** QLoRA (4-bit quantized LoRA)\n",
    "\n",
    "**Task:** Instruction-following / Customer Support Responses\n",
    "\n",
    "---\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This repository contains a **Phi-2 language model fine-tuned using QLoRA** on a synthetic dataset of customer support instructions and responses. The fine-tuning uses **4-bit quantized LoRA adapters** for memory-efficient training and can run on GPU or CPU (slower on CPU).\n",
    "\n",
    "The model is designed for **instruction-following tasks** like customer support, FAQs, or other dialog generation tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Data\n",
    "\n",
    "The fine-tuning dataset is synthetic, consisting of 3000 instruction-response pairs:\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```text\n",
    "Instruction: \"Customer asks about refund window #1\"\n",
    "Response: \"Our refund window is 30 days from delivery.\"\n",
    "```\n",
    "\n",
    "Here is the dataset that was used for fine-tunning:\n",
    "https://huggingface.co/datasets/mishrabp/customer-support-responses/resolve/main/train.csv\n",
    "\n",
    "You can replace the dataset with your own CSV/JSON file to train on real-world data.\n",
    "\n",
    "---\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "* Generate responses to instructions in customer support scenarios.\n",
    "* Small-scale instruction-following experiments.\n",
    "* Educational or research purposes.\n",
    "\n",
    "---\n",
    "\n",
    "## How to Use\n",
    "\n",
    "### Load the Fine-Tuned Model\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# -----------------------------\n",
    "# Load fine-tuned model from HF\n",
    "# -----------------------------\n",
    "model_name = \"mishrabp/phi2-qlora-finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\")\n",
    "model = PeftModel.from_pretrained(base_model, model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Sample evaluation dataset\n",
    "# -----------------------------\n",
    "eval_data = [\n",
    "    {\"instruction\": \"Customer asks about refund window\", \"reference\": \"Our refund window is 30 days from delivery.\"},\n",
    "    {\"instruction\": \"Order arrived late\", \"reference\": \"Sorry for the delay. A delivery credit has been applied.\"},\n",
    "    {\"instruction\": \"Wrong item received\", \"reference\": \"We’ll ship the correct item and provide a return label.\"},\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation loop\n",
    "# -----------------------------\n",
    "for i, example in enumerate(eval_data, 1):\n",
    "    prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "    generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Example {i}\")\n",
    "    print(\"Instruction:\", example[\"instruction\"])\n",
    "    print(\"Generated Response:\", generated.split(\"### Response:\")[-1].strip())\n",
    "    print(\"Reference Response:\", example[\"reference\"])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: compute simple token-level accuracy or BLEU\n",
    "# -----------------------------\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "bleu_scores = []\n",
    "for example in eval_data:\n",
    "    prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "    generated = tokenizer.decode(output_ids[0], skip_special_tokens=True).split(\"### Response:\")[-1].strip()\n",
    "\n",
    "    reference_tokens = example[\"reference\"].split()\n",
    "    generated_tokens = generated.split()\n",
    "    bleu = sentence_bleu([reference_tokens], generated_tokens)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "print(\"Average BLEU score:\", sum(bleu_scores)/len(bleu_scores))\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Training Script\n",
    "\n",
    "The training script performs the following steps:\n",
    "\n",
    "1. Loads the **Phi-2 base model**.\n",
    "2. Creates a **synthetic dataset** of instruction-response pairs.\n",
    "3. Tokenizes and formats the dataset for causal language modeling.\n",
    "4. Applies a **LoRA adapter**.\n",
    "5. Trains using **QLoRA** if GPU is available, otherwise full-precision LoRA on CPU.\n",
    "6. Saves the adapter and tokenizer to `./phi2-qlora`.\n",
    "7. Pushes the adapter and tokenizer to Hugging Face Hub.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "```bash\n",
    "pip install torch transformers peft datasets huggingface_hub python-dotenv\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Parameters\n",
    "\n",
    "* `r=8`, `lora_alpha=16`, `lora_dropout=0.05`\n",
    "* `target_modules=[\"q_proj\",\"v_proj\"]` (adjust for different base models)\n",
    "* Learning rate: `2e-4`\n",
    "* Batch si\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "from huggingface_hub import HfApi, Repository\n",
    "\n",
    "repo_id = \"mishrabp/phi2-qlora-finetuned\"\n",
    "\n",
    "# Option 1: Using HfApi to upload README\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"README.md\",\n",
    "    path_in_repo=\"README.md\",  # must be exactly README.md for HF Hub\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    "    token=os.environ[\"HF_TOKEN\"]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (.venv)",
   "language": "python",
   "name": "buildllm-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
